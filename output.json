["{\"title\":\"Building A Generative AI Platform\",\"short\":\"Building a robust Generative AI platform involves a multi-step process: context enhancement via RAGs, security through guardrails, optimized routing & gateways, caching strategies, and sophisticated workflows.  Observability is key throughout.\",\"long\":\"### Enhancing Context with RAGs\\nLeverage Retrieval-Augmented Generation (RAG) to provide models with relevant information from external sources like databases and the internet.  This significantly improves response quality and reduces hallucinations.\\n\\n### Implementing Guardrails\\nEssential for safety and reliability.  Input guardrails prevent sensitive data leaks and model jailbreaking, while output guardrails ensure response quality and manage failures through checks and human intervention.\\n\\n### Model Routers and Gateways\\nUse routers to direct queries to the most appropriate model, optimizing cost and performance. Gateways offer a secure and unified interface for interacting with diverse models.\\n\\n### Optimizing with Caches\\nCaching boosts speed and reduces costs. Prompt, exact, and semantic caches offer varying levels of sophistication and trade-offs between efficiency and complexity. Semantic caching, using embeddings, is the most advanced but can be unreliable.\\n\\n### Complex Logic and Write Actions\\nAdvanced applications use complex logic (loops, conditional branching) to guide model interactions.  Write actions, such as sending emails, increase capabilities but demand careful security considerations to prevent misuse.\\n\\n### Observability\\nEssential for monitoring and debugging.  Track metrics (latency, cost, response quality), maintain comprehensive logs, and use traces to follow the execution path of every query through your system.\",\"links\":[\"https://arxiv.org/abs/2005.11401\",\"https://arxiv.org/abs/1810.04805\",\"https://github.com/UKPLab/sentence-transformers\",\"https://arxiv.org/abs/1702.08734\",\"https://research.google/blog/announcing-scann-efficient-vector-similarity-search/\",\"https://github.com/spotify/annoy\",\"https://github.com/nmslib/hnswlib\",\"https://arxiv.org/abs/1603.09320\",\"https://ann-benchmarks.com/\",\"https://arxiv.org/abs/2307.03172\",\"https://www.pinecone.io/learn/chunking-strategies/\",\"https://js.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\",\"https://docs.llamaindex.ai/en/stable/optimizing/production_rag/\",\"https://www.youtube.com/watch?v=8OJC21T2SL4\",\"https://arxiv.org/abs/2303.08896\",\"https://arxiv.org/abs/2403.18802\",\"https://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt\",\"https://www.bloomberg.com/news/articles/2023-05-02/samsung-bans-chatgpt-and-other-generative-ai-use-by-staff-after-leak\",\"https://x.com/JaxWinterbourne/status/1733339886155968714\",\"https://huyenchip.com/2024/01/16/sampling.html#constraint_sampling\",\"https://github.com/Portkey-AI/gateway\",\"https://mlflow.org/docs/latest/llms/gateway/index.html\",\"https://github.com/wealthsimple/llm-gateway\",\"https://docs.truefoundry.com/docs/ai-gateway\",\"https://konghq.com/products/kong-ai-gateway\",\"https://developers.cloudflare.com/ai-gateway/\",\"https://arxiv.org/pdf/2311.04934\",\"https://ai.google.dev/gemini-api/docs/caching\",\"https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#prompt-caching\",\"https://blog.langchain.dev/announcing-langsmith/\",\"https://learning.oreilly.com/library/view/ai-engineering/9781098166298/\",\"https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ch08.html#software_system_failures\",\"https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969?&amp;_encoding=UTF8&amp;tag=chiphuyen-20&amp;linkCode=ur2&amp;linkId=0a1dbab0e76f5996e29e1a97d45f14a5&amp;camp=1789&amp;creative=9325\",\"https://arxiv.org/abs/2404.12272\",\"https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US\",\"https://medium.com/pinterest-engineering/how-we-built-text-to-sql-at-pinterest-30bad30dabff\",\"https://medium.com/vimeo-engineering-blog/from-idea-to-reality-elevating-our-customer-support-through-generative-ai-101a2c5ea680\",\"https://www.shortwave.com/blog/deep-dive-into-worlds-smartest-email-ai/\",\"https://engineering.grab.com/llm-powered-data-classification\",\"https://www.uber.com/blog/from-predictive-to-generative-ai/\",\"https://github.com/langchain-ai/langchain\",\"https://github.com/run-llama/llama_index\",\"https://github.com/FlowiseAI/Flowise\",\"https://github.com/langflow-ai/langflow\",\"https://github.com/deepset-ai/haystack\",\"https://x.com/luke_metz\",\"https://www.linkedin.com/in/findalexli/\",\"https://www.linkedin.com/in/chetantekur/\",\"https://www.linkedin.com/in/kittipat-bot-kampa-1b1965/\",\"https://www.linkedin.com/in/hienluu/\",\"https://www.linkedin.com/in/denyslinkov/\"]}", "{\"title\":\"Measuring personal growth\",\"short\":\"How do you measure personal growth? This article proposes three metrics: rate of change (how quickly you transform), time to solve problems (major life challenges), and number of future options (maximizing potential).  It's a thought-provoking look at self-assessment beyond traditional metrics.\",\"long\":\"### Measuring Personal Growth\\n\\nInspired by founder friends' focus on business growth metrics, the author explores ways to measure personal growth beyond net worth or social media followers.  Three key metrics are proposed:\\n\\n### Rate of Change\\n\\nThe author posits a 3-6 year life cycle where significant personal shifts occur.  The rate at which these transformations happen is a potential measure of personal growth, drawing a parallel to the 'rule of 72' in finance.\\n\\n### Time to Solve Problems\\n\\nThis metric centers around resolving major life challenges\u2014career, family, and finances. The author suggests that quicker resolution of these core problems indicates growth, freeing up time and energy for new pursuits.\\n\\n### Number of Future Options\\n\\nThis measure emphasizes empowerment maximization:  choosing actions that maximize future possibilities.  Growth is seen in expanding potential opportunities, rather than simply the loss of dreams. The author contrasts this with the perspective that some dreams naturally fade over time.\\n\\nThe author concludes by emphasizing the importance of personal heuristics for growth that reflect individual values and ambitions. The metrics proposed are not universally applicable but offer a framework for self-reflection.\\n\\n### Conclusion\\n\\nThe article provides a thoughtful framework for self-assessment and personal development, focusing on personal values and adaptation over time. It offers food for thought rather than prescriptive advice.\",\"links\":[\"https://twitter.com/waitbutwhy/status/1367871165319049221\"]}", "{\"title\":\"What I learned from looking at 900 most popular open source AI tools\",\"short\":\"Analyzed 900+ open-source AI repos on GitHub, revealing a booming ecosystem, especially in AI applications & engineering.  China's AI open-source scene is also rapidly growing, distinct from the West.  Highlights the potential of one-person AI companies & collaborative open-source development.\",\"long\":\"### Data Analysis of 900+ Open-Source AI Tools\\n\\nThis article details an analysis of over 900 open-source AI repositories on GitHub, focusing on tools related to foundation models. The author searched using keywords like \\\"gpt,\\\" \\\"llm,\\\" and \\\"generative ai,\\\" filtering for repositories with at least 500 stars.  The analysis reveals a rapidly evolving landscape, particularly in application development and AI engineering.\\n\\n### The New AI Stack\\n\\nThe AI stack is categorized into three layers: infrastructure (serving, compute management, vector search), model development (frameworks, optimization, evaluation), and application development (prompt engineering, RAG, interfaces).  The author also distinguishes model repositories and applications built on top of existing models. The growth of each layer is charted, showing an explosion in 2023, followed by a potential plateau.\\n\\n### Open-Source AI Developers\\n\\nA significant finding is the dominance of organizations, but there's also an emergence of highly successful individual developers creating impactful projects. The article highlights the potential for one-person billion-dollar companies in this space. The sheer volume of contributions, approaching a million commits, underscores the collaborative nature of open-source AI development.\\n\\n### China's Growing Open-Source Ecosystem\\n\\nThe study reveals a significant and distinct open-source AI ecosystem in China, with many repositories targeting Chinese audiences and utilizing models and tools tailored for Chinese language and platforms. This highlights the global diversification of the field.\\n\\n###  Repo Lifespan and Favorite Ideas\\n\\nThe author observes a \u201chype curve\u201d for many projects, with rapid initial growth followed by stagnation. However, several impactful projects remain active and offer significant contributions. The analysis concludes with the author's personal favorites including batch inference optimization, faster decoders, model merging, constrained sampling, and niche tools solving specific problems effectively. The complete list of repositories is available on the author's website.\",\"links\":[\"https://news.ycombinator.com/item?id=39709912\",\"https://www.linkedin.com/posts/chiphuyen_generativeai-aiapplications-llmops-activity-7174153467844820993-ztSE\",\"https://twitter.com/chipro/status/1768388213008445837\",\"https://huyenchip.com/llama-police\",\"https://github.com/stars/chiphuyen/lists/cool-llm-repos\",\"https://huyenchip.com/2020/06/22/mlops.html\",\"https://github.com/dair-ai/Prompt-Engineering-Guide\",\"https://github.com/f/awesome-chatgpt-prompts\",\"https://forms.gle/1ijNSnizgWQaVYK16\",\"https://github.com/vllm-project/vllm\",\"https://github.com/triton-inference-server/server\",\"https://github.com/skypilot-org/skypilot\",\"https://github.com/facebookresearch/faiss\",\"https://milvus.io/\",\"https://github.com/qdrant/qdrant\",\"https://github.com/lancedb/lancedb\",\"https://github.com/CompVis/stable-diffusion\",\"https://github.com/openai/whisper\",\"https://github.com/facebookresearch/llama\",\"https://huyenchip.com/2024/02/28/predictive-human-preference.html#correctness_of_chatbot_arena_ranking\",\"https://arxiv.org/abs/2212.09720\",\"https://arxiv.org/abs/2402.17764\",\"https://github.com/THUDM/ChatGLM3\",\"https://github.com/ymcui/Chinese-LLaMA-Alpaca\",\"https://github.com/BlinkDL/RWKV-LM\",\"https://github.com/QwenLM/Qwen\",\"https://huyenchip.com/2020/12/27/real-time-machine-learning.html#mlops_china_vs_us\",\"https://github.com/FMInference/FlexGen\",\"https://github.com/ggerganov/llama.cpp/pull/1375\",\"https://github.com/FasterDecoding/Medusa\",\"https://github.com/hao-ai-lab/LookaheadDecoding\",\"https://github.com/cg123/mergekit\",\"https://github.com/outlines-dev\",\"https://github.com/guidance-ai/guidance\",\"https://github.com/sgl-project/sglang\",\"https://github.com/arogozhnikov/einops\",\"https://github.com/huggingface/safetensors\",\"https://huyenchip.com/llama-police\",\"https://fortune.com/2024/02/04/sam-altman-one-person-unicorn-silicon-valley-founder-myth/\",\"https://www.reddit.com/r/ChatGPT/comments/1ajwj5z/one_person_billion_dollar_company/\",\"https://github.com/lucidrains\",\"https://github.com/ggerganov\",\"https://github.com/lllyasviel\",\"https://github.com/xtekky\",\"https://github.com/THUDM\",\"https://github.com/OpenGVLab\",\"https://github.com/OpenBMB\",\"https://github.com/InternLM\",\"https://github.com/open-mmlab\",\"https://github.com/QwenLM\"]}", "{\"title\":\"Predictive Human Preference: From Model Ranking to Model Routing\",\"short\":\"Predicting which LLM a user will prefer for *each* prompt is key to efficient AI. This article shows how a simple predictor, trained on crowd-sourced data, achieves 76.2% accuracy in predicting preferred LLMs, paving the way for cost-effective model routing and better AI applications.\",\"long\":\"### Predictive Human Preference in AI Model Selection\\n\\nChoosing the right AI model for a given task is a major challenge.  This article explores 'predictive human preference,' a method to predict which model a user would prefer for a specific query, eliminating the need for manual selection.  This approach is particularly useful for model routing, which dynamically assigns prompts to the most suitable (and often cheapest or fastest) model.\\n\\n###  Evaluating Model Ranking Methods\\n\\nThe article benchmarks the accuracy of LMSYS's Chatbot Arena ranking system, a popular method for comparing LLMs. Using their dataset of crowd-sourced comparisons, it finds that the higher-ranked model is preferred 74.1% of the time, highlighting the limitations of generalized rankings.\\n\\n### Building a Preference Predictor\\n\\nA machine learning model is created to predict user preference on a per-prompt basis. It takes the prompt and model pair as input and predicts the preferred model.  The initial results show a 76.2% accuracy in predicting preferred models when considering the prompt, a modest improvement over a general model ranking, but significant at scale.\\n\\n### Domain-Specific Leaderboards\\n\\nThe preference predictor allows for the creation of dynamic leaderboards based on specific queries or domains.  This offers insights into model strengths and weaknesses across different contexts, facilitating better model selection and resource allocation.  The analysis reveals that simpler prompts allow weaker models to perform almost as well as stronger ones, while more complex prompts show a clear preference for top-tier models.\\n\\n### Conclusion and Future Directions\\n\\nPredictive human preference offers a promising solution for optimizing AI model usage.  Model routing, driven by these predictions, can significantly reduce costs and latency while improving response quality. Future work will focus on enhancing the model with more data and exploring more advanced routing strategies.\",\"links\":[\"https://huyenchip.com/2023/05/02/rlhf.html\",\"https://arxiv.org/abs/2305.18290\",\"https://arena.lmsys.org/\",\"https://arxiv.org/abs/2112.00861\",\"https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=HdZrGr4IcWCl\",\"https://drive.google.com/file/d/1ZXiBRtADf9HZ8eEarIFTy-qrPNDUP_H0/view\",\"https://drive.google.com/file/d/1ZXiBRtADf9HZ8eEarIFTy-qrPNDUP_H0/view\",\"https://huggingface.co/datasets/lmsys/chatbot_arena_conversations\",\"https://techcrunch.com/2023/11/15/martians-tool-automatically-switches-between-llms-to-reduce-costs/\",\"https://discord.gg/Bgxhua5XVR\",\"https://twitter.com/luke_metz\",\"https://www.linkedin.com/in/hanchunglee/\"]}", "{\"title\":\"Sampling for Text Generation\",\"short\":\"Master AI text generation! Learn key sampling strategies (temperature, top-k, top-p), optimize with test-time sampling, and build production-ready systems with structured outputs.  Balance creativity & consistency for perfect results!\",\"long\":\"### Sampling Strategies in Text Generation\\nAI models generate text probabilistically.  Understanding sampling is crucial for managing creativity and consistency.  Techniques like temperature adjust probabilities, increasing creativity at the cost of coherence.  Top-k and top-p sampling improve efficiency and contextual relevance.\\n\\n### Test-Time Sampling for Improved Accuracy\\nGenerating multiple outputs and selecting the best one improves performance, especially for factually-driven tasks.  Choosing the highest-probability output or using a reward model are effective methods, but cost increases proportionally with the number of samples.\\n\\n### Structured Outputs for Production-Ready AI\\nStructured outputs are vital for tasks requiring specific formats (SQL, JSON) or seamless integration with downstream systems.  Prompt engineering, fine-tuning, and constraint sampling help guide models toward structured outputs; fine-tuning offers the most reliable results but requires more effort.\\n\\n### Balancing Creativity and Determinism\\nThe choice between creativity and consistency depends on the application.  High-temperature sampling prioritizes creativity but may lead to incoherent results, while lower temperatures result in more predictable, less imaginative responses.  Finding the right balance requires experimentation.\\n\\n###  Advanced Techniques and Considerations\\nAdvanced sampling techniques, like constraint sampling, aim to enforce specific grammatical or structural rules during generation. However, these often involve significant complexity, and improvements in model instruction-following might make them less critical in the future.\",\"links\":[\"https://cookbook.openai.com/examples/using_logprobs\",\"https://multithreaded.stitchfix.com/blog/2023/03/06/expert-in-the-loop-generative-ai-at-stitch-fix/\",\"https://engineering.grab.com/llm-powered-data-classification\",\"https://arxiv.org/pdf/2110.14168.pdf\",\"https://platform.openai.com/docs/guides/text-generation/json-mode\",\"https://github.com/guidance-ai/guidance\",\"https://github.com/outlines-dev/outlines\",\"https://platform.openai.com/docs/api-reference/completions/create#completions-create-best_of\",\"https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\",\"https://leehanchung.github.io/\",\"https://twitter.com/luke_metz\"]}", "{\"title\":\"Multimodality and Large Multimodal Models (LMMs)\",\"short\":\"Multimodal AI is revolutionizing how we interact with AI! Models like CLIP & Flamingo process images & text, enabling zero-shot image classification & text generation.  Future directions include handling more data types, instruction following, and generating diverse outputs (text, images, videos).\",\"long\":\"### Understanding Multimodal AI\\nMultimodal AI systems process various data types (text, images, audio) mimicking human intelligence.  This surpasses single-modality models, offering richer understanding and interaction.\\n\\n### Why Multimodality Matters\\nReal-world applications demand it. Healthcare, robotics, and e-commerce heavily rely on diverse data. Combining data types improves model performance and creates flexible interfaces (typing, voice, visuals).\\n\\n### Key Multimodal Models: CLIP and Flamingo\\nCLIP maps images and text to a shared space, excelling at zero-shot image classification. Flamingo builds upon this, adding a language model for text generation conditioned on both visual and textual inputs. \\n\\n### Emerging Trends in LMMs\\nResearch expands to more modalities (video, 3D), instruction-following models (LLaVA), and efficient training methods (adapters).  Multimodal output generation is also a key focus, enabling richer, more informative responses.\\n\\n### Conclusion\\nMultimodal AI is rapidly evolving.  While still in its early stages, its potential to transform various sectors is undeniable. LMMs are pushing the boundaries of AI capabilities.\",\"links\":[\"https://cdn.openai.com/papers/GPTV_System_Card.pdf\",\"https://www.kantar.com/uki/inspiration/advertising-media/the-power-of-tiktok\",\"https://huggingface.co/spaces/facebook/MusicGen\",\"https://arxiv.org/abs/2103.00020\",\"https://arxiv.org/abs/2204.14198\",\"https://arxiv.org/abs/2201.12086\",\"https://arxiv.org/abs/2304.08485\",\"https://openai.com/research/better-language-models\",\"https://arxiv.org/abs/1509.04942\",\"https://github.com/rom1504/clip-retrieval\",\"https://openai.com/research/dall-e\",\"https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents\",\"https://huggingface.co/spaces/HuggingFaceM4/idefics_playground\",\"https://github.com/mlfoundations/open_flamingo/issues\",\"https://arxiv.org/abs/2212.05171\",\"https://browse.arxiv.org/abs/2305.05665\",\"https://next-gpt.github.io/\",\"https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\",\"https://arxiv.org/abs/2212.10773\",\"https://arxiv.org/abs/2304.08485\",\"https://arxiv.org/abs/2305.06500\",\"https://arxiv.org/abs/2305.15023\",\"https://arxiv.org/abs/2301.12597\",\"https://arxiv.org/abs/2304.15010\",\"https://arxiv.org/abs/2201.07520\",\"https://www.linkedin.com/in/caiming-xiong-150a1417\",\"https://arxiv.org/abs/2305.17216\",\"https://arxiv.org/abs/2306.06687\",\"https://www.linkedin.com/in/hanchunglee/\",\"https://www.linkedin.com/in/samreiswig/\",\"https://twitter.com/Luke_Metz\",\"https://arxiv.org/abs/1504.00325\",\"https://arxiv.org/abs/1505.00468\",\"https://arxiv.org/abs/1904.01766\",\"https://arxiv.org/abs/1908.07490\",\"https://arxiv.org/abs/2102.02779\",\"https://arxiv.org/abs/2205.14100\",\"https://arxiv.org/abs/2302.05738\",\"https://arxiv.org/abs/2302.14045\",\"https://arxiv.org/abs/2303.03378\",\"https://arxiv.org/abs/2303.16199\",\"https://arxiv.org/abs/2304.14178\",\"https://arxiv.org/abs/2305.04160\",\"https://arxiv.org/abs/2306.15195\",\"https://arxiv.org/abs/2306.09093\",\"https://www.youtube.com/watch?v=mkI7EPD1vp8\",\"https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf\",\"https://cmu-multicomp-lab.github.io/mmml-course/fall2022/\",\"https://github.com/salesforce/LAVIS\",\"https://arxiv.org/abs/1906.06423\",\"https://www.image-net.org/challenges/LSVRC/2012/\",\"https://arxiv.org/abs/1707.02968\"]}", "{\"title\":\"Open challenges in LLM research\",\"short\":\"10 major research directions for improving LLMs: 1. Reduce hallucinations; 2. Optimize context length; 3. Multimodal data; 4. Increase speed & reduce cost; 5. New architectures; 6. GPU alternatives; 7. Build usable agents; 8. Improve learning from human preference; 9. Enhance chat interfaces; 10. Develop non-English LLMs.  #LLM #AI #Research\",\"long\":\"### Hallucinations:  LLMs sometimes fabricate information.  This is a major obstacle for production use, though for creative tasks it can be a feature.  Research focuses on reducing and measuring hallucinations, with techniques like improved prompting and self-consistency emerging. \\n\\n### Context Learning: LLMs need context for many questions.  Context length and construction are vital, especially for RAG (Retrieval Augmented Generation), impacting how efficiently models use information. The position of information within the context significantly influences model understanding.\\n\\n### Multimodality: Combining text with other data types (images, audio, etc.) enhances model performance and applicability, particularly in diverse fields like healthcare and e-commerce. It also offers a solution to the potential shortage of textual training data. \\n\\n### Efficiency:  Optimizing LLMs for speed and cost is crucial for wider adoption. Techniques like quantization and knowledge distillation are being used to create smaller, faster models that perform comparably to larger ones, enabling training on more accessible hardware. \\n\\n### New Architectures:  The Transformer architecture, though dominant, may not be optimal.  Research explores alternatives aiming for subquadratic complexity to improve efficiency and handle longer sequences.  \\n\\n### GPU Alternatives:  New hardware like TPUs, IPUs, and photonic chips are being developed to surpass the limitations of GPUs, improving speed, efficiency, and potentially enabling the use of novel architectures.\\n\\n### Agents: LLMs that can interact with the real world (e.g., browsing, sending emails) are being developed, despite challenges in reliability. These agents show promise in simulations and social behavior modeling. \\n\\n### Human Preference Learning:  Improving RLHF (Reinforcement Learning from Human Feedback) requires addressing the challenges of mathematically representing human preference, defining what constitutes 'good' AI behavior, and ensuring training data inclusivity. \\n\\n### Chat Interface: While chat is an accessible and robust interface, its limitations are being examined.  Enhancements like multiple messages per turn and multimodal input are being explored to enhance its effectiveness. \\n\\n### Non-English LLMs: Adapting LLMs to non-English languages is crucial, but faces challenges related to data availability and language-specific nuances.  Efforts are underway to address these issues and develop models that work effectively across languages.\",\"links\":[\"https://www.linkedin.com/posts/chiphuyen_llm-airesearch-generativeai-activity-7097619722363408385-s5Cp\",\"https://twitter.com/chipro/status/1691858084824838427\",\"https://huyenchip.com/2023/05/02/rlhf.html#rlhf_and_hallucination\",\"https://arxiv.org/abs/2202.03629\",\"https://arxiv.org/abs/2305.13534\",\"https://arxiv.org/abs/2302.04023\",\"https://arxiv.org/abs/2212.10400\",\"https://arxiv.org/abs/2203.11171\",\"https://arxiv.org/abs/2303.08896\",\"https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/grounding_rail/README.md#grounding-fact-checking-and-hallucination\",\"https://arxiv.org/pdf/2109.06157.pdf\",\"https://ai.google.com/research/NaturalQuestions\",\"https://arxiv.org/abs/2005.11401\",\"https://www.youtube.com/watch?v=njzB6fm0U8g\",\"https://arxiv.org/abs/2307.03172\",\"https://arxiv.org/abs/2103.00020\",\"https://arxiv.org/abs/2204.14198\",\"https://arxiv.org/abs/2301.12597\",\"https://arxiv.org/abs/2302.14045\",\"https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html\",\"https://arxiv.org/abs/2304.08485\",\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/playground/models/neva\",\"https://crfm.stanford.edu/2023/03/13/alpaca.html\",\"https://arxiv.org/abs/2305.14314\",\"https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ch07.html#model_compression\",\"https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969\",\"https://arxiv.org/abs/2111.00396\",\"https://together.ai/blog/monarch-mixer\",\"https://together.ai/blog/monarch-mixer\",\"https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/\",\"https://cloud.google.com/tpu/docs/intro-to-tpu\",\"https://www.graphcore.ai/products/ipu\",\"https://www.eetimes.com/cerebras-sells-100-million-ai-supercomputer-plans-8-more/\",\"https://spectrum.ieee.org/sambanova-ceo-ai-interview\",\"https://www.ibm.com/quantum\",\"https://www.nature.com/articles/d41586-023-00536-w\",\"https://quantumai.google/quantum-virtual-machine\",\"https://cqe.mit.edu/\",\"https://www.mpq.mpg.de/en\",\"https://chicagoquantum.org/\",\"https://quantum-roadmap.ornl.gov/\",\"https://lightmatter.co/\",\"https://ayarlabs.com/\",\"https://www.lightelligence.ai/\",\"https://www.luminous.com/\",\"https://www.nature.com/articles/s41377-022-00717-8\",\"https://github.com/Significant-Gravitas/Auto-GPT\",\"https://github.com/AntonOsika/gpt-engineer\",\"https://arxiv.org/abs/2304.03442\",\"https://www.theinformation.com/briefings/two-co-founders-of-adept-an-openai-rival-suddenly-left-to-start-another-company\",\"https://www.youtube.com/embed/a7CXIE_Gyy8\",\"https://huyenchip.com/2023/05/02/rlhf.html\",\"https://arxiv.org/abs/2212.08073\",\"https://www.deepmind.com/publications/fine-tuning-language-models-to-find-agreement-among-humans-with-diverse-preferences\",\"https://arxiv.org/abs/2203.02155\",\"https://twitter.com/jeremyphoward/status/1647763133665271808/photo/1\",\"https://austinhenley.com/blog/naturallanguageui.html\",\"https://wattenberger.com/thoughts/boo-chatbots\",\"https://arxiv.org/abs/2303.17710\",\"https://idratherbewriting.com/blog/ai-chat-interfaces-are-the-new-user-interface-for-docs\",\"https://eugeneyan.com/writing/llm-ux/\",\"http://dangrover.com/blog/2014/12/01/chinese-mobile-app-ui-trends.html\",\"https://acroll.medium.com/on-chat-as-interface-92a68d2bf854\",\"https://www.technologyreview.com/2016/04/25/8510/is-the-chatbot-trend-one-big-misunderstanding/\",\"http://dangrover.com/blog/2016/04/20/bots-wont-replace-apps.html\",\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/playground/models/neva\",\"https://www.youtube.com/watch?v=rd-J3hmycQs\",\"https://arxiv.org/abs/2304.05613\",\"https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized\",\"https://aya.for.ai/\",\"https://discord.gg/a2PCzB4AdE\",\"https://github.com/22-hours/cabrita\",\"https://github.com/LC1332/Luotuo-Chinese-LLM\",\"https://github.com/ymcui/Chinese-LLaMA-Alpaca\",\"https://github.com/Facico/Chinese-Vicuna\",\"https://arxiv.org/abs/2006.07264\",\"https://aclanthology.org/P19-1310\",\"https://arxiv.org/abs/2307.10169\"]}", "{\"title\":\"Generative AI Strategy\",\"short\":\"Need a generative AI strategy? This article summarizes a talk outlining a practical framework for exploring and implementing generative AI, emphasizing key considerations and community collaboration.\",\"long\":\"### Generative AI Strategy Framework\\nThis article summarizes a talk on developing a generative AI strategy.  It emphasizes the need for a practical framework due to the current uncertainty surrounding generative AI applications.\\n\\n### Key Considerations\\nThe framework guides exploration of generative AI, acknowledging many ideas are still evolving.  It encourages feedback and shared experiences to refine the approach.\\n\\n### Presentation Slides\\nThe article includes a link to download presentation slides which detail the framework further and provide a more visual representation of the strategy.\\n\\n### Community Feedback\\nThe author acknowledges the contributions of several individuals who provided feedback on both the presentation and the overall generative AI strategy.\\n\\n### Call to Action\\nThe article concludes by inviting readers to share their own experiences and insights, highlighting a collaborative approach to understanding and implementing generative AI.\",\"links\":[\"https://fullyconnected.com/\",\"https://huyenchip.com/assets/genai.pdf\",\"https://www.linkedin.com/feed/update/urn:li:activity:7072230577449439232/\",\"https://www.linkedin.com/in/kylegallatin/\",\"https://www.linkedin.com/in/goku/\",\"https://www.linkedin.com/in/hanchunglee/\",\"https://www.linkedin.com/in/jamiedeguerre/\"]}", "{\"title\":\"RLHF: Reinforcement Learning from Human Feedback\",\"short\":\"ChatGPT's magic isn't just scale & UX; it's also RLHF! This technique uses human feedback & reinforcement learning to fine-tune LLMs, resulting in more helpful & human-like responses.  Learn about the 3-phase training process & the challenges of addressing hallucinations in LLMs.\",\"long\":\"### RLHF: The Unsung Hero of ChatGPT\\nChatGPT's success often boils down to scale and user experience, but the technical innovation behind it is equally crucial.  Reinforcement Learning from Human Feedback (RLHF) is a prime example. It's a remarkable feat, merging reinforcement learning and NLP\u2014two fields that previously progressed independently.\\n\\n### Three Phases of ChatGPT Training\\nChatGPT's training involves three phases. First, a large language model (LLM) is pretrained on massive text data, learning language statistics. Second, supervised fine-tuning (SFT) refines the LLM using high-quality (prompt, response) pairs, guiding it towards desired outputs. Finally, RLHF further polishes the model. A reward model scores the LLM's responses, and reinforcement learning optimizes the model to maximize these scores.\\n\\n### The Role of the Reward Model\\nThe reward model (RM) is key to RLHF.  It learns to score responses based on comparison data\u2014judgments of which response is better.  Training an effective RM requires carefully curated comparisons between different responses to the same prompt.\\n\\n### Addressing Hallucination\\nHallucination, where LLMs generate fabricated information, is a major challenge. Current hypotheses suggest it stems from a lack of cause-and-effect understanding or a mismatch between the LLM's and human labeler's knowledge. RLHF, while not a perfect solution, aims to improve response quality and reduce hallucinations by using human preferences to guide the model's learning process.  However, RLHF doesn't entirely eliminate hallucinations.\",\"links\":[\"https://www.linkedin.com/posts/chiphuyen_llm-chatgpt-rlhf-activity-7059550685578608640-ap2Z/\",\"https://twitter.com/chipro/status/1653787366090510337\",\"https://openai.com/research/instruction-following\",\"https://arxiv.org/abs/1208.0984\",\"https://openai.com/research/learning-from-human-preferences\",\"https://arxiv.org/abs/2005.14165\",\"https://www.deepmind.com/publications/scaling-language-models-methods-analysis-insights-from-training-gopher\",\"https://github.com/togethercomputer/RedPajama-Data\",\"https://arxiv.org/abs/2302.13971\",\"https://arxiv.org/abs/2211.04325\",\"https://www.redditinc.com/policies/data-api-terms\",\"https://policies.stackoverflow.co/teams/enterprise-cloud-business/\",\"https://arxiv.org/abs/2203.02155\",\"https://arxiv.org/pdf/2203.02155.pdf\",\"https://arxiv.org/abs/2112.11446\",\"https://arxiv.org/abs/2204.05862\",\"https://arxiv.org/abs/2212.08073\",\"https://huggingface.co/datasets/Anthropic/hh-rlhf\",\"https://huggingface.co/datasets/databricks/databricks-dolly-15k\",\"https://projects.laion.ai/Open-Assistant/docs/data/datasets\",\"https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81\",\"https://www.youtube.com/watch?v=hhiLw5Q_UFg\",\"https://www.alignmentforum.org/posts/BgoKdAzogxmgkuuAt/behavior-cloning-is-miscalibrated\",\"https://www.youtube.com/live/hhiLw5Q_UFg?feature=share&t=1019\",\"https://arxiv.org/abs/2110.10819\",\"https://openai.com/research/openai-baselines-ppo\",\"https://openai.com/research/instruction-following\"]}", "{\"title\":\"Building LLM applications for production\",\"short\":\"Productionizing LLMs is hard!  Ambiguous natural language prompts, stochastic outputs, and high costs/latency are major hurdles.  Rigorous engineering, including prompt versioning, optimization, and testing, is essential.  Finetuning & distillation offer efficiency gains.  Embeddings & vector databases power new applications like AI assistants, chatbots, and \\\"talk-to-your-data\\\" solutions.  The future holds exciting possibilities but also challenges in maintaining compatibility and combating the challenges of rapidly evolving technology.\",\"long\":\"### Challenges in Productionizing LLMs\\nProductionizing LLMs is difficult due to the ambiguity of natural language.  Slight prompt changes can drastically alter outputs, leading to silent failures.  LLMs are stochastic, meaning consistent output for the same input isn't guaranteed, impacting user experience.\\n\\n### Mitigating Ambiguity\\nRigorous engineering is key.  Prompt evaluation involves testing the model's understanding of examples and checking for overfitting. Prompt versioning and optimization techniques like Chain-of-Thought (CoT) improve reliability but increase cost and latency.\\n\\n### Cost and Latency Concerns\\nLLM inference costs are significant, especially with long prompts and extensive outputs.  Latency increases proportionally with output length.  Cost/latency analysis is challenging due to rapidly changing API pricing and performance improvements.\\n\\n### Prompting vs. Finetuning\\nPrompting is faster for small datasets, while finetuning provides better performance and reduced costs for larger datasets. Prompt tuning offers a middle ground. Finetuning with distillation, as in the Stanford Alpaca project, leverages a larger model's outputs to train smaller, cheaper models.\\n\\n### Embeddings and Vector Databases\\nUsing LLMs to generate embeddings for search and recommendation is cost-effective.  Vector databases are crucial for low-latency retrieval, fueling the rise of this technology.\\n\\n### Backward/Forward Compatibility\\nMaintaining compatibility between prompts and different LLM versions is a challenge. Unit testing is essential to ensure prompt robustness. The evolving nature of LLMs makes prompt rewriting a likely necessity.\\n\\n### Task Composability and Agents\\nComplex applications require task composability. Agents coordinate multiple tasks using control flows (sequential, parallel, if-else, for loops), and tools (e.g., database query, web search).  Testing individual components and the overall workflow is crucial for reliable agents.\\n\\n### Promising Use Cases\\nAI assistants, chatbots, programming aids, educational tools, \\\"talk-to-your-data\\\" applications, search/recommendation systems, sales tools, and SEO are emerging use cases for LLMs.  Many applications involve breaking down complex tasks into smaller, manageable sub-tasks.\",\"links\":[\"https://news.ycombinator.com/item?id=35565212\",\"https://www.linkedin.com/posts/chiphuyen_llms-promptengineering-mlops-activity-7051955337221844992-oG7a/\",\"https://twitter.com/chipro/status/1646189847625998336\",\"https://oreillymedia.pxf.io/GmaeBn\",\"https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md#how-to-improve-reliability-on-complex-tasks\",\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\"https://arxiv.org/abs/2201.11903\",\"https://arxiv.org/abs/2203.11171\",\"https://arxiv.org/abs/2103.08493\",\"https://arxiv.org/abs/2104.08691\",\"https://github.com/tatsu-lab/stanford_alpaca\",\"https://github.com/Muennighoff/sgpt\",\"https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9\",\"https://news.ycombinator.com/item?id=36258882\",\"https://situatedqa.github.io/\",\"https://ofir.io/self-ask.pdf\",\"https://docs.google.com/spreadsheets/d/1tmfn8jKb7T1x7PpyO7rD023tH2zc_WDg_OHh0aVXIrw/edit#gid=174517450\",\"https://docs.google.com/spreadsheets/d/1GqwPo1FpAbe_awmNZW5ZMH69yc5QtEr7ZYw-ckaz_mQ/edit#gid=795016726\",\"https://quorablog.quora.com/Poe-1\",\"https://twitter.com/sharifshameem/status/1284095222939451393\",\"https://socket.dev/blog/introducing-socket-ai-chatgpt-powered-threat-analysis\",\"https://www.youtube.com/watch?v=8y7GRYaYYQg\",\"https://www.youtube.com/watch?v=-R4PWIkgOSk\",\"https://twitter.com/george__mack/status/1640010331606106112\",\"https://github.com/openai/chatgpt-retrieval-plugin\",\"https://arxiv.org/abs/2211.01910\",\"https://www.linkedin.com/feed/update/urn:li:activity:7051061478405115904/\"]}", "{\"title\":\"Snowflake RAG: Idea to prod in 10 steps\",\"short\":\"Decoding ML launches a new open-source course on production LLMs & RAG, features Shaw Talebi's AI Builders Bootcamp, and details building a production RAG app on Snowflake & H&M's fashion AI recommendation engine. #AI #LLM #RAG #MLOps #Snowflake\",\"long\":\"### New Open-Source Course on Production LLMs & RAG Systems\\nDecoding ML announces a new open-source course focused on building production-ready Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) systems.  The course draws from experience with the LLM Engineer\u2019s Handbook and LLM Twin course.\\n\\n### AI Builders Bootcamp\\nShaw Talebi, an experienced AI engineer, is launching a 7-week AI Builders Bootcamp in January.  The course emphasizes practical application through project-based learning and real-world use cases.  Example code and slides are open-sourced.  A 25% discount is available with the code PAUL25.\\n\\n### Production RAG App with Snowflake\\nThis section details building a production-ready RAG application using Snowflake's managed platform.  It outlines a 10-step process, encompassing data ingestion, retrieval, generation, and a Streamlit chatbot UI.  Snowflake handles infrastructure, reducing development time significantly.\\n\\n### H&M's Fashion AI Recommendation Engine\\nThe article explains H&M's real-time fashion recommendation engine, built using a 4-stage architecture (candidate generation, filtering, ranking, ordering).  Hopsworks, an AI Lakehouse, powers the system, providing real-time features and model management.  The article highlights the challenges of real-time recommendation systems.\",\"links\":[\"https://github.com/PacktPublishing/LLM-Engineers-Handbook\",\"https://github.com/decodingml/llm-twin-course\",\"https://www.linkedin.com/in/shawhintalebi/\",\"https://github.com/ShawhinT/AI-Builders-Bootcamp-1\",\"https://maven.com/shaw-talebi/ai-builders-bootcamp?promoCode=PAUL25\",\"https://rebrand.ly/homepage-course\",\"https://decodingml.substack.com/p/the-ultimate-recommender-system-framework\"]}", "{\"title\":\"Your RAG is wrong: Here's how to fix it\",\"short\":\"Advanced RAG: Optimize your LLM retrieval system! Master pre-retrieval, vector search, embeddings, and prompt engineering for better LLM performance. #RAG #LLM #AI\",\"long\":\"### Optimize Your RAG System\\n\\nThe standard RAG (Retrieval Augmented Generation) framework has limitations. This article explores advanced RAG techniques to optimize retrieval and answer generation.\\n\\n### Pre-retrieval Optimizations\\n\\nThis stage focuses on preparing your data for efficient retrieval. Techniques include using a sliding window for context preservation, cleaning your dataset for accuracy, adding metadata (dates, IDs) for filtering, optimizing index structures (chunk sizes), and employing a 'small-to-big' strategy that uses smaller chunks for embedding while keeping broader context for the LLM.\\n\\n### Query Optimization\\n\\nRefine user queries to better match your data. Methods include query routing (choosing the best data source and method based on query type), query rewriting (paraphrasing, synonym replacement, sub-queries), and query expansion (adding related terms).\\n\\n### Retrieval Optimizations\\n\\nImprove your embedding models by fine-tuning or using instructor models for domain-specific knowledge.  Enhance vector search with hybrid search (combining keyword and vector search) or filtered vector search (filtering via metadata before vector search).\\n\\n### Post-retrieval Optimizations\\n\\nImprove the quality of the final output. Methods include prompt compression to remove noise and re-ranking retrieved chunks with a cross-encoder model for better relevance.\\n\\n### Conclusion\\n\\nOptimizing RAG involves a multifaceted approach across data preparation, retrieval, and LLM input refinement.  The best techniques depend on your specific data and needs, requiring experimentation and iteration.\",\"links\":[\"https://huggingface.co/hkunlp/instructor-xl\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\",\"https://decodingml.substack.com/p/rag-fundamentals-first\"]}", "{\"title\":\"Great AI engineers master this... embeddings!\",\"short\":\"Level up your AI engineering skills! Master Python & cloud deployment before building ML models. Understand embeddings, the core of AI, and build a real-time personalized recommender system with our hands-on course. #AI #MLEngineering #Embeddings #MLOps\",\"long\":\"###  Mastering the Fundamentals of AI Engineering\\nBefore diving into ML models, aspiring AI engineers must master two crucial skills: programming and cloud engineering.  Proficiency in Python is a great starting point for programming, while cloud deployment skills, such as using AWS, are vital for delivering valuable projects.\\n\\n### Embeddings: The Cornerstone of AI\\nEmbeddings are essential for various AI applications, including GenAI, RAG, and recommender systems. They act as translators between real-world data (text, images, etc.) and vector representations that ML models can understand.  Similar items have vectors close together in the embedding space, enabling the discovery of powerful relationships through vector comparisons.\\n\\n### Hands-on Recommender System Course\\nLearn to build and deploy personalized recommender systems.  A new lesson explores training pipelines for real-time recommenders, using two-tower network architecture and Hopsworks feature stores. This course teaches MLOps best practices for creating and managing models.\\n\\n### Additional Resources\\nThis article provides links to further resources like a cloud engineering course, an article on embeddings, and a hands-on recommender system course.  A 10% discount code is offered for the cloud engineering course.\",\"links\":[\"https://mlops-club.org/courses-python-aws-info?am_id=decodingml\",\"https://www.linkedin.com/in/eric-riddoch/\",\"https://decodingml.substack.com/p/embeddings-the-cornerstone-of-ai\",\"https://rebrand.ly/homepage-course\",\"https://decodingml.substack.com/p/training-pipelines-for-tiktok-like\"]}", "{\"title\":\"RAG done right - Legal AI search engine case study\",\"short\":\"Building a production-ready legal AI search engine? This article shares 3 key lessons learned from the ai-aflat.ro team:  semantic search, optimal knowledge base chunking, and the importance of tailoring solutions to specific needs. #AI #LegalTech #RAG #SemanticSearch\",\"long\":\"### SaaS: Search as a Service\\nBuilding an AI search engine for legal documents requires a semantic search approach, focusing on meaning rather than keywords.  Retrieval-Augmented Generation (RAG) uses AI to generate responses based on retrieved information, ensuring accurate and controlled answers.\\n\\n### Knowledge Base Suitability\\nBefore building a semantic search engine, analyze if the AI model can capture nuances in your data.  The authors used a visual analysis of embeddings to verify that their model correctly clustered legal articles by code type.\\n\\n### Chunking for Context\\nDividing large datasets into meaningful chunks is crucial for RAG.  Simple word-count-based chunking can disrupt sentences, leading to poor results. Overlapping chunks ensure that complete sentences remain intact, minimizing context loss.  Ideal chunk size balances the embedding model's constraints with the need for sufficient context for response generation.\\n\\n### Information Size and Optimal Chunking\\nThe authors tested various chunk sizes (paragraphs, pages, chapters) to determine the best balance between minimizing false positives (small chunks) and minimizing false negatives (large chunks).  The optimal strategy depends on the expected user queries and may involve variable-length or synthetic chunks (summaries, questions, etc.).\\n\\n### Conclusion: Tailor-Made Solutions\\nCreating a truly effective RAG-based search engine requires careful consideration of many factors.  A generic, one-size-fits-all solution is unlikely to succeed.  This article's key takeaway is the critical need for customization based on the specific use case and anticipated user interactions.\",\"links\":[\"https://try.qdrant.tech/high-performance-vector-search?utm_source=google&utm_medium=cpc&utm_campaign=21518712216&utm_content=163351119977&utm_term=qdrant&hsa_acc=6907203950&hsa_cam=21518712216&hsa_grp=163351119977&hsa_ad=724496064467&hsa_src=g&hsa_tgt=kwd-1329481093586&hsa_kw=qdrant&hsa_mt=e&hsa_net=adwords&hsa_ver=3&gad_source=1&gclid=CjwKCAiA65m7BhAwEiwAAgu4JEgB07RosrFgddDCorUQxtlDkFqeQzDm5lGgRqi2Pl7HkwFzdM0PRxoCq4kQAvD_BwE\",\"https://www.linkedin.com/in/vlad-tudor-18090a1a2/\",\"https://www.linkedin.com/in/flavius-denis-moldovan-a715b0166/\"]}", "{\"title\":\"Deploy scalable TikTok-like recommenders\",\"short\":\"Deploy a scalable TikTok-like recommender! Learn to build offline & online inference pipelines using a 4-stage architecture, vector databases, Hopsworks, & KServe.  Includes a Streamlit app & GitHub Actions deployment. #MLOps #RecommendationSystems #KServe #Hopsworks\",\"long\":\"### Building a Real-time Personalized Recommender\\nThis article details deploying scalable, TikTok-like recommender systems.  It focuses on building offline and online inference pipelines for real-time personalized recommendations, using a 4-stage architecture and vector databases.\\n\\n### Offline Inference Pipeline\\nThe offline pipeline processes all candidate items in batch mode for high throughput, generating embeddings using a trained candidate article encoder. This runs periodically to update embeddings as new articles are added or the model is retrained.  The process leverages Hopsworks' feature store to ensure data consistency.\\n\\n### Online Inference Pipeline\\nThe online pipeline serves real-time recommendations.  It implements a 4-stage architecture: Candidate Generation (using customer embeddings and vector similarity search), Candidate Filtering, Ranking (scoring candidates with a ranking model), and Ordering. Hopsworks Serverless and KServe handle model deployment and scaling, optimizing for low latency.\\n\\n### Deployment with KServe\\nThe online pipeline deploys two KServe services: one for query encoding and one for ranking.  Custom Transformer and Predictor classes handle model loading and data preprocessing/postprocessing. The system is designed to scale efficiently using Kubernetes, with the ability to scale to zero instances when inactive to reduce costs.\\n\\n### Testing and Monitoring\\nA Streamlit app is provided for testing and visualizing real-time recommendations.  It interacts directly with the Hopsworks deployed models.  Deployment instructions are also given.\\n\\n### Offline Pipeline Deployment with GitHub Actions\\nGitHub Actions automate the offline ML pipelines, allowing for scheduled or triggered execution of feature generation and model training. This approach provides a robust, scalable solution for managing the offline components of the recommender system.  The benefits include free computing and enterprise-level network access, ideal for large datasets.\\n\\nThe article concludes with a live demo link and detailed step-by-step instructions.\",\"links\":[\"https://github.com/decodingml/personalized-recommender-course\",\"https://www.hopsworks.ai/dictionary/inference-pipeline?utm_source=decoding+ml+lesson+4&utm_medium=+substack+email\",\"https://www.hopsworks.ai/dictionary/kserve?utm_source=decoding+ml+lesson+4&utm_medium=+substack+email\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/notebooks/4_ip_computing_item_embeddings.ipynb\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/recsys/inference/query_transformer.py\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/recsys/inference/ranking_transformer.py\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/recsys/inference/ranking_predictor.py\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/recsys/hopsworks_integration/ranking_serving.py\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/notebooks/5_ip_creating_deployments.ipynb\",\"https://github.com/decodingml/personalized-recommender-course/actions/workflows/ml_pipelines.yaml\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/.github/workflows/ml_pipelines.yaml\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/streamlit_app.py\",\"https://rebrand.ly/homepage-lesson-4\",\"https://rebrand.ly/serverless-lesson-4\",\"https://github.com/decodingml/personalized-recommender-course?tab=readme-ov-file#-course-outline\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md#-running-the-ml-pipelines-in-github-actions\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md#%EF%B8%8F-deploying-the-streamlit-app\",\"https://decodingml-hands-on-personalized-recommender.streamlit.app/\",\"https://rebrand.ly/hands-on-recsys-github\"]}", "{\"title\":\"SLM Fine-tuning: Top Results at 9.47% Cost\",\"short\":\"Boost your MLOps skills with a live Databricks course, master cost-effective SLM fine-tuning with Snowflake, and build a real-time H&M recommender system! #MLOps #LLMs #RecommenderSystems\",\"long\":\"### Speed up your MLOps learning experience\\n\\nWant to accelerate your MLOps journey?  Check out the End-to-End MLOps with Databricks course, starting January 27th. Taught by experienced engineers Maria Vechtomova and Ba\u015fak Tu\u011f\u00e7e Eskili, founders of MarvelousMLOps, this live course covers everything from MLOps principles to Databricks mastery. Get 10% off with code: PAUL.\\n\\n### Fine-tuning SLMs for top-tier results\\n\\nAchieve top-tier results on specialized tasks with SLMs, at a significantly reduced cost.  This tutorial details how to fine-tune smaller LLMs using knowledge distillation and Snowflake, providing an end-to-end solution. It's all about striking the right balance between accuracy and cost-effectiveness. \\n\\n### Build an H&M Personalized Recommender\\n\\nLearn to build a real-time, personalized recommender system for H&M. Lesson 2 of the Hands-on H&M Real-Time Personalized Recommender course focuses on creating feature pipelines using best MLOps practices. Taught by Paolo Perrone, this course is perfect for data scientists, ML engineers, and anyone passionate about recommender systems.  Learn to process datasets, engineer features, and leverage Hopsworks for training-serving skew solutions.\",\"links\":[\"https://maven.com/s/course/cd68987afa\",\"https://rebrand.ly/homepage-course\",\"https://github.com/decodingml/llm-twin-course\",\"https://github.com/decodingml/personalized-recommender-course\",\"https://github.com/decodingml/information-retrieval-tutorials\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\"]}", "{\"title\":\"Forget text-to-SQL: Use this natural query instead\",\"short\":\"Forget text-to-SQL! Build a powerful Amazon product search using natural queries, LLMs, Superlinked, MongoDB's vector database, and OpenAI.  This article shows how to create a multi-attribute tabular semantic search engine for e-commerce.\",\"long\":\"### Modernizing Search with Tabular Semantic Search\\nThis article explores building a tabular semantic search for Amazon products, moving beyond keyword-based searches.  Instead of text-to-SQL, it uses embeddings for all product attributes (text, numbers, categories). \\n\\n### Superlinked and MongoDB for Multi-Attribute Search\\nSuperlinked is an open-source framework that handles complex data structures. It efficiently manages vector embeddings and queries across various data types within a single index. MongoDB's Atlas vector search is used as a scalable vector database.\\n\\n### Processing and Embedding the Amazon Dataset\\nThe Amazon ESCI dataset (a sample of ~4400 products) is cleaned, and its attributes are embedded using techniques suitable for each data type (text using Alibaba-NLP/gte-large-en-v1.5).  Numerical values are embedded to retain semantic relationships.\\n\\n### Natural Language Queries\\nNatural language queries are used, leveraging an LLM (OpenAI) and Superlinked to automatically decode them into structured queries and filters against the multi-attribute vector index.  This avoids manual SQL query generation.\\n\\n### RESTful API and Streamlit UI\\nThe retrieval system is wrapped as a RESTful API using Superlinked's RestExecutor.  A Streamlit app provides a user interface for interacting with the API and testing different queries.\\n\\n### Conclusion\\nThis approach demonstrates a powerful alternative to text-to-SQL, enabling more natural and accurate e-commerce searches. The flexibility of Superlinked and the scalability of MongoDB Atlas make this solution suitable for real-world applications.\",\"links\":[\"https://rebrand.ly/amazon-tabular-semantic-github\",\"https://github.com/shuttie/esci-s?tab=readme-ov-file\",\"https://rebrand.ly/superlinked-nlq-notebook\",\"https://www.mongodb.com/products/platform/atlas-vector-search\",\"https://rebrand.ly/superlinked-homepage\",\"https://docs.superlinked.com/concepts/multiple-embeddings\",\"https://docs.superlinked.com/concepts/dynamic-parameters\",\"https://superlinked.com/vectorhub/articles/multi-attribute-semantic-search\",\"https://rebrand.ly/mongo-atlas-quick-start\",\"https://rebrand.ly/amazon-tabular-semantic-eda-nb\",\"https://github.com/superlinked/superlinked/tree/main?tab=readme-ov-file#overview\",\"https://www.gnu.org/software/make/\",\"https://rebrand.ly/mongo-atlas\"]}", "{\"title\":\"Structure your Python AI code like a pro\",\"short\":\"Level up your Python AI code! Learn to build real-time personalized recommenders using a 4-stage architecture and DDD for modular, scalable code.  Get a 7-day free trial of CodeCrafters via my link!\",\"long\":\"### AI/ML Engineering: A Blend of Software and AI\\nAI/ML engineering effectively combines software engineering (SWE) and AI expertise.  Learning SWE involves building applications of varying complexity. CodeCrafters offers an interactive platform to build projects in Python, Rust, and Java, covering tools like Redis, Shell, and Kafka.\\n\\n### Real-time Personalized Recommenders: The 4-Stage Architecture\\nReal-time recommenders face the challenge of filtering millions of items to a personalized selection within a second.  A 4-stage architecture addresses this, using an AI lakehouse like Hopsworks. It involves an offline pipeline (candidate generation) and an online pipeline (ranking). The online pipeline has four stages: candidate generation (reducing millions of items to hundreds), filtering (removing seen or purchased items), ranking (scoring candidates using features from Hopsworks), and ordering (final ranking with optional business rules). This process ensures personalized recommendations in real-time.\\n\\n### Structuring Python ML Code with SWE Best Practices\\nStructuring Python ML code requires applying SWE best practices for modularity, extendability, and scalability. Domain-Driven Design (DDD) offers a framework for organizing files, classes, and interfaces.  DDD's four main components are domain (core entities), application (business logic), infrastructure (concrete implementations), and interfaces (interaction layers).  This approach allows easy code modification, testing, and infrastructure changes. Applying DDD concepts helps improve the structure of ML projects, such as LLM and RAG applications, making them more maintainable and scalable.\",\"links\":[\"https://app.codecrafters.io/join?via=iusztinpaul\",\"https://rebrand.ly/homepage-course\",\"https://github.com/PacktPublishing/LLM-Engineers-Handbook\",\"https://rebrand.ly/hands-on-recsys-github\",\"https://decodingml.substack.com/p/the-ultimate-recommender-system-framework\"]}", "{\"title\":\"Training pipelines for TikTok-like recommenders\",\"short\":\"Build a TikTok-like recommender! This lesson covers training pipelines using two-tower architecture, Hopsworks feature views, and a ranking model.  Learn to train, evaluate, and deploy models efficiently using MLOps best practices. #MLOps #RecommenderSystems #TikTok\",\"long\":\"### Two-Tower Architecture\\nThis architecture efficiently narrows down millions of items to a few hundred relevant candidates. It consists of two parallel models: the Query Tower (Customer Query Encoder) and the Candidate Tower (Articles Encoder), creating embeddings in a shared vector space for efficient retrieval.\\n\\n### Hopsworks Retrieval Feature View\\nThis crucial step combines user, item, and interaction data from various feature groups into a unified dataset. It prevents training/inference skew and centralizes features for efficient reuse across models.\\n\\n### Training the Two-Tower Network\\nThe training involves a forward pass computing user and item embeddings, loss calculation (retrieval and regularization), gradient computation using Gradient Tape, and weight updates with the AdamW optimizer.  Top-100 accuracy evaluates the model's ability to retrieve relevant items.\\n\\n### Ranking Model\\nThis model refines recommendations by assigning relevance scores.  The CatBoostClassifier is used for its efficiency with categorical features and built-in support for early stopping and feature importance.\\n\\n### Hopsworks Model Registry\\nTrained models (two-tower and ranking) are saved here for easy deployment. The registry offers flexible storage, performance tracking, and seamless integration with Hopsworks' serving infrastructure.\\n\\n### Running the Training Pipeline\\nThe pipeline involves creating a Hopsworks account, running the feature pipeline to populate feature groups, and then running the training pipeline.  Instructions are provided for local notebooks, CLI, and GitHub Actions (recommended for slow connections).\",\"links\":[\"https://github.com/decodingml/personalized-recommender-course\",\"https://rebrand.ly/hands-on-recsys-github\",\"https://www.hopsworks.ai/dictionary/training-pipeline?utm_source=decoding+ml+lesson+3&utm_medium=+substack+email\",\"https://docs.hopsworks.ai/latest/concepts/mlops/registry/?utm_source=decoding+ml+lesson+3&utm_medium=+substack+email\",\"https://www.hopsworks.ai/dictionary/two-tower-embedding-model?utm_source=decoding+ml+lesson+3&utm_medium=+substack+email\",\"https://cloud.google.com/blog/products/ai-machine-learning/scaling-deep-retrieval-tensorflow-two-towers-architecture\",\"https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems\",\"https://www.evidentlyai.com/ranking-metrics/ndcg-metric\",\"https://github.com/decodingml/hands-on-personalized-recommender/blob/main/recsys/training/two_tower.py\",\"https://github.com/decodingml/hands-on-recommender-systems/blob/main/recsys/hopsworks_integration/feature_store.py\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/recsys/training/ranking.py\",\"https://github.com/decodingml/hands-on-personalized-recommender/blob/main/recsys/hopsworks_integration/two_tower_serving.py\",\"https://decodingml.substack.com/p/33d3273e-b8e3-4d98-b160-c3d239343022\",\"https://decodingml.substack.com/p/0d113ac4-c84e-4144-a78a-fc0cc9dd34b2\",\"https://decodingml.substack.com/p/deploy-scalable-tiktok-like-recommenders\",\"https://rebrand.ly/serverless-lesson-3\",\"https://github.com/decodingml/personalized-recommender-course?tab=readme-ov-file#-course-outline\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md#-running-the-ml-pipelines-in-github-actions\",\"https://docs.hopsworks.ai/latest/concepts/fs/feature_view/fv_overview/?utm_source=decoding+ml+lesson+3&utm_medium=+substack+email\"]}", "{\"title\":\"Build a TikTok-like recommender from scratch!\",\"short\":\"Boost your ML skills! Learn about maximizing model accuracy, RAG fundamentals, and building a TikTok-like recommender system. Free course available on GitHub!\",\"long\":\"### Quick questions from Decoding ML\\nWe post diverse content on Substack's social feed, blog, and newsletter articles.  Your feedback helps us improve.  Please take our quick poll!\\n\\n### This week's topics:\\nThis week's content covers maximizing model accuracy (webinar with Tecton's CTO), RAG fundamentals (why use RAG instead of fine-tuning LLMs), and building a TikTok-like recommender system from scratch.\\n\\n### Maximizing Model Accuracy\\n91% of models degrade in production! Join our webinar on December 17th to learn how to maximize model accuracy in production.  We'll discuss common issues and architectural patterns for solutions. \\n\\n### RAG Fundamentals\\nRetrieval-Augmented Generation (RAG) improves LLM accuracy by fetching relevant information from external sources.  This tackles hallucinations and outdated/private information issues. Read our in-depth article on RAG fundamentals. \\n\\n### Build a TikTok-like Recommender\\nBuild a real-time personalized recommender system using H&M data.  Our free 5-lesson course covers ML system design, architecture, MLOps best practices, building a 4-stage recommender, a two-tower model and an LLM-enhanced system. Get started on GitHub!\",\"links\":[\"https://resources.tecton.ai/maximizing-model-accuracy-strategies-from-ml-experts?utm_campaign=%5BFY25Q4%5D%20-%20Maximizing%20Model%20Accuracy%3A%20Strategies%20from%20ML%20Experts&utm_source=Decoding%20ML\",\"https://rebrand.ly/homepage-course\",\"https://rebrand.ly/hands-on-recsys-github\",\"https://decodingml.substack.com/p/33d3273e-b8e3-4d98-b160-c3d239343022\",\"https://decodingml.substack.com/p/rag-fundamentals-first\"]}", "{\"title\":\"Building feature pipelines for TikTok-like recommenders\",\"short\":\"Build a real-time personalized recommender for H&M using a 4-stage architecture, two-tower model, and Hopsworks.  This lesson focuses on feature pipelines, including batch and streaming processes, and the power of Hopsworks Feature Groups for efficient feature management and model reproducibility.\",\"long\":\"### H&M Dataset\\nThis lesson uses the H&M dataset, comprising 'articles', 'customers', and 'transactions' tables.  Relationships between tables are crucial for feature engineering.  Customer IDs link to transaction data for behavioral features (purchase frequency, recency). Article IDs in transactions reveal product popularity and customer preferences.\\n\\n### Feature Engineering for Two-Tower Model\\nThe two-tower model learns user and item embeddings for efficient retrieval.  Key features include `customer_id`, `age`, `month_sin`, `month_cos` (query features), and `article_id`, `garment_group_name`, `index_group_name` (candidate features). A minimal feature set is chosen for fast retrieval.\\n\\n### Feature Engineering for Ranking Model\\nThe ranking model predicts purchase probability.  It uses query features (same as retrieval) and item features (detailed attributes, color, department) along with a binary label (1 for purchase, 0 for negative sample).  Negative sampling ensures realistic training data.\\n\\n### Hopsworks Feature Groups\\nHopsworks Feature Groups manage features efficiently.  This system creates Feature Groups for articles, customers, transactions, and ranking, establishing data lineage. Version control and metadata management ensure reproducibility.\\n\\n### Streaming Data Pipeline\\nFor real-time recommendations, integrate streaming data (user behavior, product updates, embedding updates).  The retrieval stage upgrades to an incremental ANN index, handling live embedding updates. The filtering stage uses stream processing to dynamically adjust recommendations based on fresh data (user activity, stock levels).\\n\\n### Running the Feature Pipeline\\nRun the pipeline locally (Jupyter Notebook), via the CLI, or using GitHub Actions (recommended for poor internet connections).  Hopsworks hosts the Feature Groups.\",\"links\":[\"https://github.com/decodingml/personalized-recommender-course\",\"https://github.com/decodingml/hands-on-recommender-system\",\"https://github.com/decodingml/hands-on-personalized-recommender\",\"https://www.hopsworks.ai/dictionary/feature-pipeline?utm_source=decoding+ml+lesson+2&utm_medium=+substack+email\",\"https://docs.hopsworks.ai/latest/concepts/fs/feature_group/fg_overview/?utm_source=decoding+ml+lesson+2&utm_medium=+substack+email\",\"https://docs.hopsworks.ai/latest/concepts/fs/feature_view/fv_overview/?utm_source=decoding+ml+lesson+2&utm_medium=+substack+email\",\"https://www.hopsworks.ai/post/data-validation-for-enterprise-ai-using-great-expectations-with-hopsworks?utm_source=decoding+ml+lesson+2&utm_medium=newsletter\",\"https://rebrand.ly/serverless-lesson-2\",\"https://github.com/decodingml/personalized-recommender-course?tab=readme-ov-file#-course-outline\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md#-running-the-ml-pipelines-in-github-actions\",\"https://decodingml.substack.com/p/33d3273e-b8e3-4d98-b160-c3d239343022\",\"https://decodingml.substack.com/p/training-pipelines-for-tiktok-like\",\"https://decodingml.substack.com/p/deploy-scalable-tiktok-like-recommenders\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\",\"https://github.com/decodingml/llm-twin-course\",\"https://github.com/decodingml/information-retrieval-tutorials\",\"https://doi.org/10.1145/3285029\",\"https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/\"]}", "{\"title\":\"LLM Prompt Monitoring: Building Production Pipelines\",\"short\":\"Master LLM prompt monitoring! This article shows how to build production-ready pipelines using Opik, tracking prompts, latency, and evaluating key metrics like hallucinations and moderation for robust LLMOps.  #LLMOps #PromptEngineering #MLOps\",\"long\":\"### Monitoring LLM Applications\\nThe article emphasizes the importance of specialized software for monitoring Large Language Model (LLM) applications, particularly focusing on prompt monitoring.  It highlights the challenges of tracking latency metrics in streaming outputs, including Time to First Token (TTFT), Time Between Tokens (TBT), Tokens per Second (TPS), Time per Output Token (TPOT), and Total Latency.  Accurate tracking of input and output tokens is crucial for cost management.\\n\\n### Implementing Prompt Monitoring\\nThe article details implementing prompt monitoring using Opik, an open-source Python tool.  It demonstrates how to use the `@opik.track` decorator to log LLM calls and integrate with LangChain for more complex RAG setups.  The process involves attaching relevant metadata, including token counts, model IDs, and prompt variables, for detailed analysis.  This provides a comprehensive trace of each step, facilitating easier debugging.\\n\\n### Evaluating LLMs in Production\\nThe article explains the process of evaluating LLM performance in a production environment.  Since ground truth data might be unavailable, the focus shifts to metrics like Hallucination, Moderation, Answer Relevance, and Style.  It recommends random sampling of production data to make evaluations more efficient.  This data is used to build a monitoring evaluation pipeline, triggered periodically or upon reaching a specified sample size, allowing for proactive identification and response to issues.\\n\\n### Practical Application and Conclusion\\nThe article walks through the steps of deploying the monitoring and evaluation pipeline, using a step-by-step approach and make commands. It emphasizes the significance of logging prompts and their traces for effective debugging and performance assessment. The conclusion summarizes the key learnings on building a robust prompt monitoring service and evaluation pipeline. It also points towards next steps, encouraging readers to extend their knowledge through the provided resources, like the LLM Twin open-source course and related books.\",\"links\":[\"https://github.com/decodingml/llm-twin-course\",\"https://www.comet.com/opik?utm_source=decoding_ml&utm_medium=partner&utm_content=substack\",\"https://www.comet.com/signup/?utm_source=decoding_ml&utm_medium=partner&utm_content=substack\",\"https://open.substack.com/pub/decodingml/p/the-ultimate-llm-fine-tuning-pipeline?r=1ttoeh&utm_campaign=post&utm_medium=web\",\"https://decodingml.substack.com/p/6370c679-10b9-4944-a4f1-4228e3ab2d60\",\"https://open.substack.com/pub/decodingml/p/beyond-pocs-building-rag-systems?r=1ttoeh&utm_campaign=post&utm_medium=web\",\"https://github.com/decodingml/llm-twin-course?tab=readme-ov-file#lessons\",\"https://medium.com/decodingml/the-engineers-framework-for-llm-rag-evaluation-59897381c326\",\"https://medium.com/decodingml/beyond-proof-of-concept-building-rag-systems-that-scale-e537d0eb063a\",\"https://www.comet.com/docs/opik/cookbook/quickstart_notebook?utm_source=decoding_ml&utm_medium=partner&utm_content=substack\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/inference_pipeline/llm_twin.py\",\"https://github.com/decodingml/llm-twin-course/blob/main/INSTALL_AND_USAGE.md\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/inference_pipeline/evaluation/evaluate_monitoring.py\"]}", "{\"title\":\"Building a TikTok-like recommender\",\"short\":\"Learn to build a TikTok-like real-time personalized recommender for H&M fashion! This system uses a 4-stage architecture, a 2-tower model, and Hopsworks for efficient scaling to millions of items.  #recsys #AI #MLOps #MachineLearning\",\"long\":\"### Building a TikTok-like Recommender\\nThis article details the architecture of a real-time personalized recommender system, similar to TikTok's, but applied to H&M fashion items.  It leverages a four-stage architecture and a two-tower embedding model.\\n\\n### Two-Tower Embedding Model\\nA two-tower model creates embeddings for customers and items in the same vector space. This allows for efficient retrieval of relevant items by calculating the distance between customer and item embeddings.\\n\\n### Four-Stage Recommender Architecture\\nThe system uses a four-stage architecture: Candidate Generation, Filtering, Ranking, and Re-ranking.  Candidate generation uses coarse features for broad personalization. Filtering removes irrelevant items, ranking refines the list with more detailed features using models like CatBoost, and re-ranking applies business logic for final ordering.\\n\\n### Hopsworks Integration\\nThe system utilizes Hopsworks, an AI Lakehouse, for feature storage, model registry, and real-time serving.  This facilitates efficient data management and model deployment.\\n\\n### Feature/Training/Inference (FTI) Architecture\\nThe FTI architecture organizes the system into three pipelines: Feature, Training, and Inference. This modular approach simplifies building and deploying production-ready ML systems.\\n\\n### GitHub Actions Deployment\\nOffline pipelines (feature, training, embedding generation) are deployed using GitHub Actions for automation and scalability.\\n\\n### Streamlit Application\\nA Streamlit application provides a simple frontend for interacting with the real-time recommender system, showcasing its functionality.\",\"links\":[\"https://github.com/decodingml/personalized-recommender-course\",\"https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/overview\",\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf\",\"https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage\",\"https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines\",\"https://decodingml.substack.com/p/building-ml-systems-the-right-way\",\"https://developer.nvidia.com/merlin\",\"https://cloud.google.com/blog/products/ai-machine-learning/scaling-deep-retrieval-tensorflow-two-towers-architecture\",\"https://docs.hopsworks.ai/latest/concepts/fs/feature_group/fg_overview/?utm_source=decoding+ml+lesson+1&utm_medium=newsletter\",\"https://docs.hopsworks.ai/latest/concepts/fs/feature_view/fv_overview/?utm_source=decoding+ml+lesson+1&utm_medium=newsletter\",\"https://www.hopsworks.ai/dictionary/feature-store?utm_source=decoding+ml+lesson+1&utm_medium=newsletter\",\"https://www.hopsworks.ai/dictionary/ml-pipeline?utm_source=decoding+ml+lesson+1&utm_medium=newsletter\",\"https://github.com/features/actions\",\"https://github.com/streamlit\",\"https://streamlit.io/cloud\",\"https://rebrand.ly/homepage-lesson-1\",\"https://rebrand.ly/feature-store-lesson-1\",\"https://rebrand.ly/ml-pipelines-lesson-1\",\"https://github.com/decodingml/personalized-recommender-course/blob/main/INSTALL_AND_USAGE.md\",\"https://decodingml.substack.com/p/feature-pipeline-for-tiktok-like\"]}", "{\"title\":\"Beyond PoCs: Building RAG systems that scale\",\"short\":\"Build scalable & cost-effective production RAG systems using a microservices architecture with AWS SageMaker & Hugging Face TGI.  Deploy your fine-tuned LLM and integrate it with a Gradio chatbot UI! #LLMs #RAG #AWS #SageMaker #MLOps\",\"long\":\"###  Remastered LLM Twin Course\\nThe open-source LLM Twin course has been significantly improved with simplified code, bug fixes, and new lessons.  These include fine-tuning with Unsloth and AWS SageMaker, LLM & RAG evaluation with Opik, deploying the fine-tuned LLM to AWS SageMaker, and prompt monitoring with Opik.  All updated lessons are available on GitHub.\\n\\n### Production RAG System Architecture\\nThe article details building a production-ready RAG system using a microservices architecture. This separates the LLM service (inference, GPU-intensive) from the business logic service (RAG retrieval, context processing, CPU-based). This modular design enables independent scaling, optimizing resource usage and cost.  The interface uses a REST API for communication.\\n\\n### AWS SageMaker Deployment\\nDeploying the fine-tuned LLM to AWS SageMaker is described step-by-step, leveraging Hugging Face's Text Generation Inference (TGI) server.  TGI offers features like tensor parallelism, token streaming, and quantization for optimized performance and reduced VRAM needs. A g5.2xlarge instance with an A10G GPU is suggested for an 8B parameter LLM with quantization.\\n\\n### RAG Business Module and Chatbot UI\\nThe business logic module is implemented as a Python class (`LLMTwin`), handling RAG and interaction with the SageMaker endpoint.  A chatbot UI is created using Gradio, providing an accessible interface to interact with the deployed model.  The process includes creating prompt templates with support for both RAG and simple prompt interactions.  The article shows how to run the entire pipeline locally and provides commands for deploying it on AWS. \\n\\n### Training vs. Inference Pipelines\\nKey differences between training and inference pipelines are discussed, focusing on data access methods (offline batch vs. online low-latency), infrastructure requirements (powerful GPU machines for training vs. more efficient CPU-based resources for inference), and the importance of matching preprocessing and postprocessing steps between training and inference to avoid training-serving skew.\",\"links\":[\"https://github.com/decodingml/llm-twin-course\",\"https://huggingface.co/docs/text-generation-inference/en/index\",\"https://huggingface.co/docs/sagemaker/en/index\",\"https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f#2ba3\",\"https://medium.com/decodingml/sota-python-streaming-pipelines-for-fine-tuning-llms-and-rag-in-real-time-82eb07795b87\",\"https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2\",\"https://medium.com/decodingml/how-to-fine-tune-llms-on-custom-datasets-at-scale-using-qwak-and-cometml-12216a777c34\",\"https://qdrant.tech/?utm_source=decodingml&utm_medium=referral&utm_campaign=llm-course\",\"https://huggingface.co/pauliusztin/LLMTwin-Meta-Llama-3.1-8B\",\"https://github.com/HazyResearch/flash-attention\",\"https://github.com/vllm-project/vllm\",\"https://github.com/TimDettmers/bitsandbytes\",\"https://arxiv.org/abs/2210.17323\",\"https://github.com/comet-ml/opik\",\"https://medium.com/decodingml/the-role-of-feature-stores-in-fine-tuning-llms-22bd60afd4b9\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/inference_pipeline/aws/deploy_sagemaker_endpoint.py\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/inference_pipeline/llm_twin.py\",\"https://github.com/decodingml/llm-twin-course/blob/main/INSTALL_AND_USAGE.md\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/inference_pipeline/ui.py\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\"]}", "{\"title\":\"LLM & RAG Evaluation Framework: A Complete Guide\",\"short\":\"Remastered open-source LLM Twin course! New lessons on fine-tuning, LLM & RAG evaluation with Opik (using LLM judges!), and SageMaker deployment.  Build production-grade pipelines & measure what matters!\",\"long\":\"### Remastered LLM Twin Course\\nThe open-source LLM Twin course has been remastered with simplified code, bug fixes, and new lessons on fine-tuning with Unsloth and AWS SageMaker, LLM & RAG evaluation with Opik, deployment to AWS SageMaker, and prompt monitoring.\\n\\n### LLM & RAG Evaluation with Opik\\nLearn to evaluate fine-tuned LLMs and RAG pipelines using Opik, an open-source tool.  This involves using heuristics, similarity scores (like BERT Score), and LLMs as judges to assess aspects like hallucination, moderation, and writing style.\\n\\n### Opik's Evaluation Framework\\nOpik provides a framework for creating evaluation datasets from Comet ML artifacts.  It offers various metrics, including custom ones, to assess LLMs and RAG systems, considering input, output, context, and ground truth.  The process involves defining evaluation tasks and scoring metrics, then visualizing the results in Opik's dashboard.\\n\\n### Running the Evaluation\\nTo run the evaluation, ensure your Docker infrastructure and Qdrant vector database are running. Deploy the LLM to SageMaker and then execute the evaluation scripts (`make evaluate-llm` and `make evaluate-rag`).  Analyze the results in the Opik dashboard for insights into model performance.\\n\\n### Further Improvements\\nFurther optimization involves collecting more data, improving data cleaning and augmentation, hyperparameter tuning, and batching predictions for efficiency.  The course provides a solid foundation for iterative improvement and experimentation.\",\"links\":[\"https://github.com/decodingml/llm-twin-course\",\"https://open.substack.com/pub/decodingml/p/the-ultimate-llm-fine-tuning-pipeline?r=1ttoeh&utm_campaign=post&utm_medium=web\",\"https://github.com/decodingml/llm-twin-course?tab=readme-ov-file#lessons\",\"https://github.com/comet-ml/opik\",\"https://www.comet.com/?utm_source=decoding_ml&utm_medium=partner&utm_content=medium\",\"https://huggingface.co/spaces/evaluate-metric/bertscore\",\"http://Levenshtein%20distance\",\"https://en.wikipedia.org/wiki/BLEU\",\"https://github.com/BerriAI/litellm\",\"http://Overview%20|%20OPIK%20Documentation.%20%28n.d.%29.%20https://www.comet.com/docs/opik/evaluation/metrics/overview\",\"http://Using%20Ragas%20to%20evaluate%20RAG%20pipelines%20|%20Opik%20Documentation.%20%28n.d.%29.%20https://www.comet.com/docs/opik/cookbook/ragas\",\"https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-processor\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/core/opik_utils.py\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/inference_pipeline/evaluation/evaluate.py\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/inference_pipeline/evaluation/evaluate_rag.py\",\"https://www.comet.com/opik\",\"https://github.com/decodingml/llm-twin-course/blob/main/INSTALL_AND_USAGE.md\",\"https://github.com/vllm-project/vllm\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\"]}", "{\"title\":\"The ultimate LLM fine-tuning pipeline\",\"short\":\"Master production-ready LLM fine-tuning! This guide uses Unsloth & AWS SageMaker for efficient, cost-effective scaling. Cut costs by 77% and optimize memory by 60%. #LLM #finetuning #MLOps #SageMaker #Unsloth\",\"long\":\"### LLM Fine-tuning Guide: Unsloth + SageMaker\\nThis article details a production-grade LLM fine-tuning pipeline using Unsloth and AWS SageMaker.  It emphasizes cost-effectiveness and scalability.\\n\\n### Mastering Production-Ready Fine-tuning\\nLearn how to fine-tune open-source LLMs from Hugging Face using Unsloth, TRL, AWS SageMaker, and Comet ML.  The focus is on building scalable and reproducible pipelines using LLMOps and SWE best practices.\\n\\n###  Efficient Fine-tuning with Unsloth and TRL\\nUnsloth and TRL are used to optimize VRAM usage and speed up training.  The article demonstrates how to use LoRA for supervised fine-tuning (SFT), resulting in a significant reduction in training costs and time.\\n\\n###  Comet ML for Experiment Tracking\\nComet ML is integrated to track experiments, log training metrics, and compare results across different runs. This facilitates choosing the best model for production deployment.\\n\\n### Scaling with AWS SageMaker\\nThe article shows how to leverage AWS SageMaker to scale the fine-tuning process to handle larger datasets and automate training.  The cost savings achieved by using Unsloth are highlighted.\\n\\n### Model Registry and Data Versioning\\nThe fine-tuned LLM is stored and versioned in the Hugging Face model registry for easy access and reproducibility.  The importance of versioning training data using Comet ML artifacts is also discussed.\\n\\n### Conclusion and Next Steps\\nThe guide provides a comprehensive workflow for LLM fine-tuning, focusing on practical engineering aspects. The next lessons will cover LLM and RAG evaluation and deployment to AWS SageMaker.\",\"links\":[\"https://github.com/SylphAI-Inc/LLM-engineer-handbook\",\"https://github.com/decodingml/llm-twin-course\",\"https://www.comet.com/?utm_source=decoding_ml&utm_medium=partner&utm_content=medium\",\"https://www.comet.com/decodingml/llm-twin/4e649019cdbb49e1967b5f1b33ff9c2d?compareXAxis=step&experiment-tab=panels&showOutliers=true&smoothing=0&xAxis=step&utm_source=decoding_ml&utm_medium=partner&utm_content=medium\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/training_pipeline/finetune.py\",\"https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html\",\"https://huggingface.co/pauliusztin/LLMTwin-Meta-Llama-3.1-8B\",\"https://github.com/decodingml/llm-twin-course/blob/main/src/training_pipeline/run_on_sagemaker.py\",\"https://aws.amazon.com/sagemaker/pricing/\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\"]}", "{\"title\":\"Connecting the dots in data and AI systems\",\"short\":\"Simplify your ML systems with the FTI (Feature, Training, Inference) architecture!  Learn how to build a real-time RAG application using an LLM fine-tuned on your own data.  #ML #AI #LLM #RAG #MLOps #FTI\",\"long\":\"### The Mess of Data and AI Systems\\nData and AI systems are often complex and disjointed, making it hard to understand how data engineering, research, and production teams work together.  The article introduces the FTI architecture as a solution.\\n\\n### The FTI Architecture\\nThe FTI (Feature, Training, Inference) architecture simplifies ML systems by breaking them down into three core pipelines.  Each pipeline has a defined scope and interface (feature store and model registry), making the entire system more manageable and scalable.  Pipelines can be developed and scaled independently.\\n\\n### LLM Twin Use Case\\nA practical example is presented: building an LLM Twin, a real-time RAG application fine-tuned on personal social media data.  The system adds a data collection pipeline (ETL) alongside the FTI pipelines.\\n\\n### Data Collection Pipeline\\nThis pipeline crawls data from sources like Medium, LinkedIn, and GitHub, standardizes it, and loads it into a NoSQL database (acting as a data warehouse).\\n\\n### Feature Pipeline\\nThis pipeline processes the data (cleaning, chunking, embedding), storing results in a logical feature store (using a vector DB for efficient retrieval by the inference pipeline).\\n\\n### Training Pipeline\\nThe training pipeline fine-tunes an LLM with instruction data from the feature store, using an experiment tracker.  The best model is then stored in the model registry.\\n\\n### Inference Pipeline\\nFinally, the inference pipeline serves predictions in real-time via a REST API, leveraging the fine-tuned LLM and the vector DB for RAG. A monitoring system tracks prompts and responses.\\n\\n### Summary\\nThe FTI architecture provides a clear, modular approach to building complex ML systems, as shown in the LLM Twin example.  The book, \\\"LLM Engineer\u2019s Handbook,\\\" offers a deeper dive into this architecture and LLM/RAG engineering.\",\"links\":[\"https://track.presspool.ai/?utm_source=466&utm_medium=cpc&utm_id=281\",\"https://decodingml.substack.com/p/building-ml-systems-the-right-way\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\"]}", "{\"title\":\"Fundamentals of ML serving: Key deployment types\",\"short\":\"Master ML model deployment! Learn the 4 key requirements (throughput, latency, data, infrastructure) and 3 core architectures (online, asynchronous, offline batch). Choose the right fit for your AI app & optimize user experience. #ML #MLOps #AI\",\"long\":\"### Four Requirements for Deploying ML Models\\nEvery ML application needs to meet specific requirements: throughput (requests per second), latency (response time), data (input/output format and volume), and infrastructure (hardware, software, networking).  Balancing these trade-offs is critical for user experience.\\n\\n### Trade-offs Between Latency and Throughput\\nThere's always a trade-off between low latency (fast responses) and high throughput (many requests). High throughput needs scalable infrastructure (multiple GPUs), while low latency requires optimized infrastructure (faster processors, edge computing).  Batching requests can complicate this balance; optimizing for throughput might increase latency and vice versa.\\n\\n### Core ML Serving Architectures\\nThree main architectures exist: online real-time inference (immediate responses, suitable for chatbots), asynchronous inference (requests queued, more efficient resource use, handles spikes), and offline batch transform (large data processing, low cost, delayed predictions, suitable for data analysis).\\n\\n### Choosing the Right ML Serving Method\\nSelection depends on requirements: high throughput with permissive latency (offline batch), low latency prioritized (online real-time), efficient resource use with acceptable latency (asynchronous).  Consider user interaction and prediction freshness.  Real-time needs low latency, while batch processing is cost-effective for non-real-time tasks.\",\"links\":[\"https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/mobile-site-load-time-statistics/\",\"https://app.codecrafters.io/join?via=iusztinpaul\",\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\"]}", "{\"title\":\"The ABCs of Diffusion Models\",\"short\":\"Diffusion models are revolutionizing AI, generating high-quality images & videos from noise. Learn the process, training, inference, and latest models like Sora, Stable Diffusion 3, and Imagen 3!\",\"long\":\"### Introduction to Diffusion Models\\nDiffusion models are a new approach in generative AI, creating high-quality images and animations by reversing a noise process.  They offer advantages over GANs and VAEs, such as producing higher quality and more diverse outputs.\\n\\n### The Diffusion Process\\nThis involves two phases: forward diffusion (adding noise to an image until it's pure noise) and reverse diffusion (using a U-Net or transformer-based model to remove noise and generate an image).  Different model types exist, such as score-based, latent diffusion, and discrete diffusion models.\\n\\n### Training and Inference\\nTraining involves predicting noise, calculating loss, and backpropagating to adjust the model's weights.  Inference uses the trained model to generate images from random noise. Metrics like FID and IS evaluate image quality and diversity.\\n\\n### Latest Advances\\nRecent advancements include Diffusion Transformer (DiT) models like Sora (text-to-video) and Stable Diffusion 3 (improved image quality and safety features). Imagen 3 (from Google) generates photorealistic images using latent diffusion. Lumiere focuses on text-to-video generation using a Space-Time U-Net.\",\"links\":[\"https://www.eventbrite.ca/e/5th-ann-mlops-world-and-generative-ai-world-conference-tickets-755502136227?discount=PAUL15\",\"https://www.linkedin.com/in/maxime-labonne/\",\"https://www.linkedin.com/in/gizdarski/\",\"https://www.linkedin.com/in/zainhas/\",\"https://mlopsworld.com/speakers/#agenda\",\"https://medium.com/decodingml/the-abcs-of-diffusion-models-51902a331068\",\"https://github.com/decodingml/articles-code/tree/main/articles/generative_ai/diffusion_models_fundamentals\"]}", "{\"title\":\"LLM Engineer's Handbook is finally live!\",\"short\":\"The LLM Engineer's Handbook is finally here! Co-authored with Maxime Labonne, this book provides a hands-on framework for building production-ready LLM & RAG apps.  Learn Data, GenAI, and Systems & MLOps best practices in one complete project! #LLM #RAG #GenAI #MLOps #AI\",\"long\":\"### LLM Engineer's Handbook Launch!\\n\\nThe author excitedly announces the release of their *LLM Engineer's Handbook*, co-authored with Maxime Labonne.  This book provides a practical framework for building LLM and RAG applications, focusing on a production-level, end-to-end approach.\\n\\n###  Hardcore Development Process\\n\\nThe book's creation involved an intensive four-month period of coding and writing, adapting to the rapidly evolving AI landscape.  The authors emphasize the challenging process, highlighting the dedication required to produce a high-quality resource.\\n\\n###  Unique Approach\\n\\nUnlike many books focusing on theory, the *LLM Engineer's Handbook* prioritizes practical application.  It guides readers through building a complete GenAI product, demonstrating the interplay between data engineering, software engineering, AI, and MLOps.\\n\\n###  Three Main Sections\\n\\nThe book is organized into three sections: **Data** (data collection, cleaning, feature engineering, dataset creation for fine-tuning, and populating a vector database), **GenAI** (fine-tuning LLMs with Unsloth, advanced RAG techniques), and **Systems & MLOps** (system design, deployment to AWS SageMaker, pipeline orchestration with ZenML, prompt monitoring with Opik from Comet ML).\\n\\n###  Support the Authors\\n\\nReaders are encouraged to support the authors by purchasing the book via the provided Amazon link.  The book is described as containing many colored diagrams and providing a comprehensive, connected project for building an LLM-powered application.  The authors express their satisfaction with the collaborative process, praising Maxime's expertise in fine-tuning LLMs.\",\"links\":[\"https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/\",\"https://www.linkedin.com/in/maxime-labonne/\",\"https://github.com/decodingml/llm-twin-course\",\"https://github.com/decodingml/personalized-recommender-course\",\"https://github.com/decodingml/information-retrieval-tutorials\"]}", "{\"title\":\"Building AI Agents from scratch - Part 2: Reflection and Working Memory\",\"short\":\"Build smarter AI agents with Reflection! This article shows how to implement reflection and working memory to improve agent planning and accuracy.  Check out the code and video tutorial!\",\"long\":\"### Reflection in AI Agents\\nThe ability of an AI agent to review its own output and suggest improvements, optionally enhancing future actions based on feedback.\\n\\n### Reflection and Short-Term Memory\\nReflection often requires short-term memory (working memory) to store previous interactions (system prompt, user query, generated plan).  This context is essential for effective reflection and improvement.\\n\\n### Simple Reflection Example\\nA user query goes to an LLM. The generated answer is then given to the LLM for feedback and improvement suggestions, resulting in a refined answer to the user. This improves accuracy over prompt engineering alone.\\n\\n### Reflection Loop\\nAn iterative process where the LLM repeatedly refines the answer, providing feedback and suggesting improvements until no further enhancements are possible.  Useful for code generation but expensive for general use.\\n\\n### Validating Execution Plans\\nReflection can validate the plan created by the agent before execution. This prevents errors, such as incorrect tool parameters, which can halt the flow.\\n\\n### Building a Reflection Agent\\nA practical example uses an agent to plan currency conversion.  The reflection step corrects an error in the initial plan (incorrect source currency), improving the accuracy of the final output.\\n\\n### Implementing Working Memory\\nWorking memory is implemented using a simple list to store interactions (timestamps, queries, plans). This context is crucial for the reflection step to function.\\n\\n### Agent Implementation\\nThe Agent class manages the interaction history, planning process, and reflection loop. The `execute` method orchestrates the overall flow, enabling plan revision based on reflection feedback.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/\",\"https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part\",\"https://github.com/swirl-ai/ai-angineers-handbook\",\"https://www.youtube.com/@swirlai\",\"https://www.newsletter.swirlai.com/i/153986635/defining-reflection-in-ai-agents\",\"https://www.newsletter.swirlai.com/i/153986635/the-simplest-example\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa642fd3b-a4bb-4465-bc4b-77948c14edea_1234x704.png\",\"https://www.newsletter.swirlai.com/i/153986635/reflection-loop\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff19163d0-6ead-4bdb-95d5-fafd5acef297_1234x704.png\",\"https://www.newsletter.swirlai.com/i/153433846/crafting-the-system-prompt\",\"https://www.newsletter.swirlai.com/i/153986635/validating-execution-plans\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe45a3fb8-84cd-4775-9b5f-421632628181_1194x828.png\",\"https://www.newsletter.swirlai.com/i/153986635/more-complex-reflection-flows\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42f7fa39-9c99-41a7-a1fd-badcf622a31c_1704x930.png\",\"https://www.newsletter.swirlai.com/i/153986635/connection-between-agent-memory-and-reflection-pattern\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b23ab7a-979f-4ba1-acdd-acaca8871040_1194x913.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F397c1965-c739-4d9b-a821-a06a817e4130_2383x1276.png\",\"https://www.newsletter.swirlai.com/i/153986635/pros-and-cons-of-using-reflection\",\"https://www.newsletter.swirlai.com/i/153986635/building-the-reflection-agent\",\"https://github.com/swirl-ai/ai-angineers-handbook/tree/main/building_agents_from_scratch/planning/reflection\",\"https://github.com/swirl-ai/ai-angineers-handbook/blob/main/building_agents_from_scratch/planning/reflection/notebooks/reflection.ipynb\",\"https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part\",\"https://www.newsletter.swirlai.com/i/153986635/implementing-the-working-memory\",\"https://www.newsletter.swirlai.com/i/153986635/the-initial-system-prompt\",\"https://www.newsletter.swirlai.com/i/153986635/implementing-the-agent-class\",\"https://www.newsletter.swirlai.com/i/153986635/reflecting-on-the-plan\",\"https://www.newsletter.swirlai.com/i/153986635/executing-the-flow-and-generating-revised-plan\",\"https://www.newsletter.swirlai.com/i/153986635/executing-the-agent\",\"https://www.newsletter.swirlai.com/i/153986635/thats-it-for-today-weve-learned\",\"https://www.newsletter.swirlai.com/p/sponsorships\",\"https://www.newsletter.swirlai.com/p/what-is-ai-engineering?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNDEyMjI1OSwicG9zdF9pZCI6MTUxNzczMTkwLCJpYXQiOjE3MzQ3NDY1MTEsImV4cCI6MTczNzMzODUxMSwiaXNzIjoicHViLTExNDQxNzEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.ozYMMhYkRStOvKkGl3QkPp9i4bSdxZ1NzUbWlfhxm98\"]}", "{\"title\":\"Building AI Agents from scratch - Part 1: Tool use\",\"short\":\"Building AI agents from scratch! Learn to create agents capable of tool use without frameworks.  This article covers tool decorators, system prompt engineering, and agent class implementation using Python and GPT. #AI #Agents #LLM #Python\",\"long\":\"### What are AI Agents?\\nAn AI agent uses an LLM to decide on actions to fulfill user intent.  It involves planning, memory (short-term and long-term, often in system prompts), and tools (functions, databases, other agents).\\n\\n### How Tool Usage Works\\nLLMs don't run code; they generate instructions.  Effective prompt engineering is crucial, especially system prompts defining tool capabilities and expected outputs.\\n\\n### Building a Tool Decorator\\nThe article shows how to create a Python decorator to wrap functions, extracting name, description (from docstrings), callable function, and parameters for LLM use.  A specific docstring format is required for parameter extraction.\\n\\n### Currency Exchange Tool Example\\nA `convert_currency` tool is built using a function and the decorator. This function fetches exchange rates and performs conversions.  This is used to illustrate how to create a tool and feed it to the agent.\\n\\n### Agent Class Implementation\\nA comprehensive `Agent` class is implemented, including methods to add tools, retrieve them, and execute them. The `create_system_prompt` method constructs the JSON system prompt for the LLM, detailing agent capabilities, instructions, and tool definitions.\\n\\n### Running the Agent\\nThe article concludes by showing how to initialize the agent, add the `convert_currency` tool, and run it with example user queries, demonstrating both tool-usage scenarios and direct responses.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://github.com/swirl-ai/ai-angineers-handbook\",\"https://github.com/swirl-ai/ai-angineers-handbook/tree/main/building_agents_from_scratch/tool_use\",\"https://github.com/swirl-ai/ai-angineers-handbook/blob/main/building_agents_from_scratch/tool_use/notebooks/tool_use.ipynb\",\"https://www.youtube.com/@swirlai\",\"https://www.newsletter.swirlai.com/p/memory-in-agent-systems\",\"https://www.newsletter.swirlai.com/p/what-is-ai-engineering?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNDEyMjI1OSwicG9zdF9pZCI6MTUxNzczMTkwLCJpYXQiOjE3MzQ3NDY1MTEsImV4cCI6MTczNzMzODUxMSwiaXNzIjoicHViLTExNDQxNzEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.ozYMMhYkRStOvKkGl3QkPp9i4bSdxZ1NzUbWlfhxm98\"]}", "{\"title\":\"AI Clouds and their role in the AI era\",\"short\":\"Build your own AI chatbot on Nebius AI Cloud using Mistral-7B! Learn about AI Clouds vs proprietary APIs and optimize your LLM costs.  #AICloud #LLM #Nebius #Mistral7B #Chatbot\",\"long\":\"### AI Clouds: The New Cloud for the AI Era\\nAI Clouds are emerging to meet the surging demand for GPU-optimized resources needed for AI applications.  They provide an alternative to using third-party LLM APIs, offering more control and cost-effectiveness at scale.\\n\\n### Hands-on Chatbot Project\\nThis article walks you through building a Mistral-7B-powered chatbot on Nebius AI Cloud. The process involves setting up a Kubernetes cluster, deploying the open-source LLM using vLLM, creating a Streamlit-based chatbot interface, and exposing the chatbot externally.\\n\\n### TCO of LLM Inference\\nDeploying your own LLMs on an AI Cloud offers a fixed cost, linearly scaling with GPU usage. Autoscaling can optimize costs.  Proprietary APIs have fluctuating costs depending on token usage, making AI Clouds more cost-effective at scale for high-throughput applications.\\n\\n### Throughput, Latency, and Autoscaling\\nAI Clouds give granular control over throughput and latency, potentially reducing Time-To-First-Token (TTFT) compared to APIs. Autoscaling in AI Clouds dynamically adjusts resources based on demand, providing responsiveness and efficiency.\\n\\n### AI Cloud vs. Proprietary APIs\\nAI Clouds are ideal for training and finetuning, while proprietary APIs are suitable for prototyping. For high-throughput production applications, AI Clouds offer a more cost-effective solution.  Understanding your Total Cost of Ownership (TCO) is vital in choosing the right inference method.\",\"links\":[\"https://docs.nebius.com/kubernetes/quickstart/#env-install\",\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://nebius.com/explorer-tier?utm_medium=newsletter&utm_source=ag&utm_campaign=explorer-tier\",\"https://nebius.com/explorer-tier?utm_medium=newsletter&utm_source=ag&utm_campaign=explorer-tier\",\"https://nebius.com/explorer-tier?utm_medium=newsletter&utm_source=ag&utm_campaign=explorer-tier\"]}", "{\"title\":\"What is AI Engineering?\",\"short\":\"AI Engineering is evolving rapidly!  It blends ML & Software Engineering to build robust, agentic AI systems using LLMs.  The future is bright with high demand and lucrative salaries. #AI #LLM #MLOps #SoftwareEngineering #AIjobs\",\"long\":\"### Evolution of AI Systems with LLMs\\nLarge Language Models (LLMs) haven't revolutionized AI systems; instead, they've added capabilities like planning, content extraction/generation, and code generation, enhancing existing AI pipelines.  AI systems remain complex, multi-component entities.\\n\\n### AI Engineering vs. ML/Software Engineering\\nAI Engineering bridges the gap between ML and Software Engineering. While building LLM applications seems simple initially, maintaining robust, scalable, and observable systems requires specialized skills.  AI Engineers deal with non-deterministic systems, demanding a blend of ML and software expertise.\\n\\n### Essential Skills for AI Engineers\\nSuccess requires research skills (understanding and applying LLM-related research papers), prompt engineering proficiency (crafting and evaluating prompts for agentic systems), strong software development and infrastructure knowledge (handling data sources and deploying scalable systems), and MLOps/AgentOps expertise (evaluating and observing non-deterministic systems).\\n\\n### The Future of AI Engineering\\nThe demand for AI Engineers will skyrocket.  Agentic systems will automate processes across industries, requiring AI Engineers' expertise in building and maintaining these complex systems. Full-stack AI Engineers will be especially valuable, driving innovation and disrupting the market.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://arxiv.org/abs/2411.10541\",\"https://www.anthropic.com/news/contextual-retrieval\",\"https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline\",\"https://www.newsletter.swirlai.com/p/sponsorships\",\"https://www.newsletter.swirlai.com/p/what-is-ai-engineering?utm_source=substack&utm_medium=email&utm_content=share&action=share\"]}", "{\"title\":\"Memory in Agent Systems\",\"short\":\"Agent systems rely on memory for effective planning and decision-making.  This article explores short-term and long-term memory, including episodic, semantic, and procedural types.  Efficient memory management is crucial, particularly as LLMs have context window limitations and cost considerations. #AI #Agents #LLMs #GenerativeAI\",\"long\":\"### Memory in Agent Systems\\nThis article explores the implementation of memory in Generative AI systems, focusing on the memory component of agents.  Agentic memory is divided into short-term (working) and long-term memory, mirroring human memory. Short-term memory, crucial for immediate context, is limited by LLM context windows and computational costs.  Continuous updates to the system prompt are inefficient and expensive.\\n\\n### Long-Term Memory\\nLong-term memory overcomes short-term limitations, storing past interactions and external knowledge.  It's categorized into:\\n\\n*   **Episodic Memory:** Stores past agent interactions and actions, acting similarly to a Retrieval Augmented Generation (RAG) system but internally within the agent. This solves issues with context window size and allows for session resumption. \\n*   **Semantic Memory:** Includes external knowledge, agent self-knowledge, and grounding context (web-scale data used in LLM training) to ensure grounded actions. Access happens through tools specified in the system prompt. \\n*   **Procedural Memory:** This is codified information like system prompts, tools, guardrails, and agent system topology.  It's crucial for guiding agent actions.\\n\\n### Short-Term Memory\\nShort-term memory provides immediate context to the agent via a system prompt.  Challenges involve managing context window size, LLM processing limitations as data size increases, and the associated computational costs. Long-term memory is designed to reduce these issues.  \\n\\n### Conclusion\\nEfficient memory management is key to building effective AI agents.  Understanding memory types and their implementations within different frameworks prevents unexpected issues. The field is evolving rapidly, and the author anticipates further developments.\",\"links\":[\"https://arxiv.org/pdf/2309.02427\",\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://home.mlops.community/public/events/aiagentsinprod\",\"https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline\"]}", "{\"title\":\"Observability in LLMOps pipeline - Different Levels of Scale\",\"short\":\"SwirlAI Newsletter returns! This issue explores the rapidly scaling observability infrastructure needs of LLMOps pipelines, from foundational model training to complex multi-agent systems.  Tracing and evaluation become crucial for managing the non-determinism and costs of advanced GenAI applications.\",\"long\":\"### Re-introduction of SwirlAI Newsletter\\nAfter a year-long break, the SwirlAI Newsletter is back, focusing on GenAI Systems Engineering, MLOps, Data Engineering for Machine Learning and GenAI, System Design, and AI news.\\n\\n### Observability in LLMOps\\nThe article discusses the increasing scale of infrastructure needed for observing processes in LLMOps pipelines, particularly in GenAI Systems Engineering.  It highlights the evolution of observability from experiment trackers for regular ML models to big data analytics platforms for LLMs.\\n\\n### GenAI Value Chain\\nThe GenAI value chain is split into foundation model training (pre-training and post-training) and GenAI system engineering (fine-tuned models, RAG, agents, and multi-agent networks).  Observability needs differ significantly across these stages.\\n\\n### RAG Systems\\nObservability in RAG systems focuses on tracing and evaluation.  The article details the components of a trace (controller, spans) and stresses the importance of tracing for error detection, cost estimation, and system evaluation.\\n\\n### Agents\\nLLM-based agents introduce more non-determinism, requiring dynamic trace generation and evaluation.  The complexity increases with long-term memory and multiple tools.\\n\\n### Multi-Agent Systems\\nMulti-agent systems present the highest level of complexity due to distributed tracing and inter-agent communication.  Observability challenges include connecting traces, managing communication patterns, and preventing infinite loops.\\n\\n### Conclusion\\nThe evolution of GenAI necessitates a significant upgrade in observability infrastructure.  Traditional ML tools are insufficient for the scale and complexity of modern GenAI systems.  The shift is towards big data analytics platforms capable of handling large, dynamic, and non-deterministic traces.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://github.com/swirl-ai/ai-angineers-handbook\",\"https://www.youtube.com/@swirlai\",\"https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline/comments\"]}", "{\"title\":\"A Guide to Kubernetes (Part 2): Different ways to deploy your application.\",\"short\":\"Learn about different ways to deploy apps on Kubernetes: Deployments for continuous apps, Jobs for finite tasks, and CronJobs for scheduled jobs.  YAML examples included! #kubernetes #devops #dataengineering\",\"long\":\"### Kubernetes Application Deployment Methods\\nThis article explores various Kubernetes resources for deploying applications, focusing on Deployments, Jobs, and CronJobs.  It builds upon a previous article covering basic Kubernetes concepts.\\n\\n### Deployments for Continuous Operation\\nDeployments manage multiple identical Pod replicas, ensuring a specified number always runs.  They handle new version releases and rollbacks gracefully.  Ideal for stateless microservices and streaming applications with automated ID handling.\\n\\n### Jobs for Finite Workloads\\nJobs are designed for applications with finite tasks, such as data extraction or database updates.  They specify the number of successful Pod completions, parallelism, and retries.  Useful for ETL jobs, schema updates, and cleanup tasks. \\n\\n### CronJobs for Scheduled Tasks\\nCronJobs automate the creation of Jobs on a schedule defined using cron syntax.  Perfect for regularly recurring tasks like daily data pipelines or scheduled database maintenance.\\n\\n###  Example Implementations\\nThe article provides YAML examples for creating and managing Deployments, Jobs, and CronJobs.  It uses a simple Python script and a nginx container to illustrate these concepts.\\n\\n### Practical Application\\nUnderstanding different deployment methods is crucial for scaling and managing diverse Kubernetes workloads effectively.  Choosing the right resource depends on the application's lifecycle and requirements. The article explains when Deployments are sufficient and when Jobs/CronJobs are better choices for various real-world scenarios.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/a-guide-to-kubernetes-part-1\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project-9fe\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project-9fe\"]}", "{\"title\":\"SAI Notes #10: Data Contracts in the Data Pipeline.\",\"short\":\"SAI Notes #10 covers Data Contracts in data pipelines, improving data quality & scalability.  Includes links to past episodes & announces new newsletter discounts for lower-income countries & students!\",\"long\":\"### Episodes you might have missed.\\nThis section lists previous newsletters, including \\\"The SwirlAI Data Engineering Project Master Template,\\\" a guide to optimizing Spark application performance (Part 2), and a guide to Kubernetes (Part 1).\\n\\n### Newsletter Discounts.\\nDiscounts are available for those in lower-income countries and students with academic email addresses. Contact the author for more information.\\n\\n### Data Contracts in the Data Pipeline.\\nA data contract defines the structure, SLAs, and semantics of data shared between producers and consumers. It ensures data quality, prevents pipeline outages, and improves scalability.  An example implementation using a Git repository, Kafka topics, and Flink applications for validation is described.  The author will delve deeper into data contracts in a future newsletter.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/8d5f7dc3\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project\",\"https://www.newsletter.swirlai.com/p/a-guide-to-optimising-your-spark-841\",\"https://www.newsletter.swirlai.com/p/a-guide-to-kubernetes-part-1\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project-9fe\",\"https://www.newsletter.swirlai.com/p/sai-notes-09-database-sharding?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNDEyMjI1OSwicG9zdF9pZCI6MTM1NTUxMDYwLCJpYXQiOjE2OTM0OTczODksImV4cCI6MTY5NjA4OTM4OSwiaXNzIjoicHViLTExNDQxNzEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.NusiXCgS5H3P6et3ukuM3WE6Y_b-MLQPlf4Ofr7IePA\"]}", "{\"title\":\"The SwirlAI Data Engineering Project Master Template: The Collector (Part 1).\",\"short\":\"Build a high-availability data collector with FastAPI & deploy it to Kubernetes! This 1st part covers implementation and deployment, using TLC Trip Record data as a producer. Part 2: code improvements, unit tests, CI/CD, and Kafka.\",\"long\":\"### SwirlAI Data Engineering Project: The Collector (Part 1)\\nThis article details building and deploying a Collector application for the SwirlAI Data Engineering Project Master Template on a local Kubernetes cluster.  The template features a real-time pipeline (Data Producers, Collector, Enricher/Validator, Enrichment API, Real-Time Loader, Batch Loader) and a batch pipeline.  The infrastructure uses Kafka, MinIO, Airflow, Presto/Trino, Elasticsearch, and Superset.\\n\\n### Collector Implementation\\nThe Collector is a FastAPI-based REST API application.  It validates incoming events (checking for `event_type`, `schema_version`, and `payload`), adds metadata (`collector_timestamp`, `root_id`, `collector_id`), and logs the processed data.  The API is versioned (`/api/v1/collect`) for future compatibility.\\n\\n### Data Producer Emulation\\nThree Python applications simulate Data Producers by downloading TLC Trip Record datasets (January, February, March) and sending individual records to the Collector API.  The download location is specified via an environment variable.\\n\\n### Kubernetes Deployment\\nThe Collector is deployed as a Kubernetes Deployment with multiple replicas for high availability.  A LoadBalancer service exposes the API.  Producers are deployed as individual Pods, each with its environment variables to download different datasets.\\n\\n### Next Steps\\nFuture articles will cover code improvements, unit testing, CI/CD with GitHub Actions, and Kafka integration.\",\"links\":[\"https://github.com/AurimasGr/data-engineering-project-master-template\",\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/a-guide-to-kubernetes-part-1\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project\",\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\",\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\",\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\"]}", "{\"title\":\"A Guide to Kubernetes (Part 1)\",\"short\":\"Master Kubernetes for Data Engineering & ML! This guide covers Namespaces, ConfigMaps, Secrets, Pods, Deployments, and Services for seamless application deployment and scaling.  Learn the theory and get ready for hands-on projects!\",\"long\":\"### Why Kubernetes for Data Professionals?\\nKubernetes simplifies the deployment and scaling of data applications, from real-time ML APIs to batch jobs and streaming applications.  Its declarative interface streamlines configuration, while its self-healing capabilities ensure high availability.  This is crucial for data engineers and ML engineers needing robust and scalable infrastructure. \\n\\n### Key Kubernetes Resources\\nNamespaces logically group resources, providing isolation and access control.  ConfigMaps store configurations, and Secrets securely manage sensitive information like passwords and credentials.  These resources simplify environment management and deployment configurations.\\n\\n### Deploying Applications with Kubernetes\\nPods encapsulate one or more containers. Deployments manage multiple identical pod replicas, ensuring scalability and self-healing. They simplify application updates and rollbacks. Services act as internal load balancers, distributing traffic across pods within a deployment.\\n\\n### Practical Steps for Getting Started\\nThis tutorial assumes you have access to a Kubernetes cluster (local or remote).  Docker and kubectl are also required. The instructions show how to set up Namespaces, ConfigMaps, Secrets, Pods, Deployments and Services to create a basic Kubernetes application architecture.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://docs.docker.com/desktop/kubernetes/\",\"https://minikube.sigs.k8s.io/docs/start/\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project\",\"https://www.newsletter.swirlai.com/p/a-guide-to-kubernetes-part-2-different\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project\"]}", "{\"title\":\"A Guide to Optimising your Spark Application Performance (Part 2)\",\"short\":\"Optimize your Spark app! Part 2 focuses on data storage & infrastructure: Choose the right file format (Parquet for OLAP, Avro for OLTP), understand Parquet encoding (RLE, Dictionary), tune executor memory, and maximize executor count while considering CPU/memory limitations.\",\"long\":\"### Choosing the Right File Format\\nWhen working with Spark and large datasets, selecting the right file format is crucial for performance.  Consider columnar storage (Parquet, ORC) for read-heavy workloads (OLAP) and row-based storage (Avro) for frequent writes (OLTP).\\n\\n### Column vs. Row Based Storage\\nColumnar storage stores data by column, making it efficient for reading subsets of columns. Row-based storage stores data row-by-row, better for writing entire records but slower for partial data reads. Parquet and ORC are columnar, while Avro is row-based.\\n\\n### Splittable vs. Non-Splittable Files\\nSplittable files (like Parquet, Avro, CSV) can be read in parallel by multiple cores, maximizing CPU utilization. Non-splittable files (JSON, XML) can only be read sequentially, limiting parallelism and increasing processing time. \\n\\n### Encoding in Parquet\\nParquet uses encoding techniques (RLE, Dictionary Encoding) for compression. RLE is effective for ordered columns with repeating values, while Dictionary Encoding works for low cardinality columns regardless of order. Combining both provides optimal compression.\\n\\n### Understanding and Tuning Spark Executor Memory\\n`spark.executor.memory` determines executor memory.  It's divided into Reserved Memory (fixed), User Memory (UDFs), and Spark Unified Memory (further split into Storage and Execution Memory). Adjusting `spark.memory.fraction` and `spark.memory.storageFraction` allows for fine-tuning this distribution.\\n\\n### Maximising the Number of Executors\\nMaximize cluster resources by considering both memory and CPU.  Don't exceed available memory per node (`yarn.nodemanager.resource.memory-mb`), and consider that leaving some cores for system processes enhances stability.  Aim for fewer than 5 cores per executor to avoid bottlenecks.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/a-guide-to-optimising-your-spark\",\"https://www.newsletter.swirlai.com/i/132368852/maximising-parallelism\",\"https://aws.amazon.com/ec2/instance-types/\",\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html#emr-hadoop-task-config-m5\",\"https://www.newsletter.swirlai.com/i/132368852/maximising-parallelism\"]}", "{\"title\":\"SAI Notes #09: Database Sharding.\",\"short\":\"Missed this month's SwirlAI Newsletters? Catch up on Spark optimization, data freshness, and MLOps! Next week: Advanced Spark tuning. Plus, database sharding explained and referral program details!\",\"long\":\"### Episodes You Might Have Missed\\nThis month's newsletters covered Spark application optimization (Part 1), data freshness in machine learning, the SwirlAI data engineering project template, and the evolving maturity of the MLOps stack.\\n\\n### Spark Optimization Guide (Part 2)\\nNext Wednesday's newsletter will delve into Spark optimization, focusing on `spark.default.parallelism`, file formats (columnar vs. row-based), Parquet encoding, data preparation, sorting, executor memory, and maximizing executors.\\n\\n### Database Sharding Explained\\nDatabase sharding, a horizontal scaling technique, is crucial for handling massive datasets. It involves splitting a database into smaller, manageable shards distributed across multiple servers. This contrasts with vertical scaling, which involves increasing a single server's resources.\\n\\n### Sharding Strategies\\nThree main sharding strategies are key-based (using hash functions for balanced distribution), range-based (dividing data into value ranges), and directory-based (predefined shards for specific key values).  Each approach has its trade-offs regarding complexity, data skew, and operational efficiency.\\n\\n### Newsletter Referrals\\nRefer friends to the SwirlAI Newsletter and earn complimentary paid subscriptions!\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/a-guide-to-optimising-your-spark\",\"https://www.newsletter.swirlai.com/p/levels-of-data-freshness-in-machine\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project\",\"https://www.newsletter.swirlai.com/p/evolving-maturity-of-mlops-stack\",\"https://www.newsletter.swirlai.com/leaderboard\"]}", "{\"title\":\"Evolving Maturity of MLOps Stack in your Organisation.\",\"short\":\"Evolving MLOps maturity: Start simple, add ML Metadata Store, automate model delivery, implement CI/CD for ML pipelines & model deployment, and finally, consider real-time monitoring & Feature Store.  Prioritize speed for early prototypes, then add complexity as needed.\",\"long\":\"### The Beginning of the Machine Learning Journey\\n\\nTypically, ML implementation starts with a Data Scientist identifying problems solvable with ML or tackling a specific business challenge. The initial system is simple, lacking automation, as MLOps practices are introduced later to optimize mature projects.  Data is analyzed, and basic ML training pipelines are run ad-hoc.  No model handover occurs until model viability is proven offline.\\n\\n### Ready for Deployment\\n\\nAfter offline validation, the model is deployed. This often involves 'throwing the model over the wall'\u2014a basic handover, such as placing model binaries in an S3 bucket for Software Engineers to deploy.  The SE handles deployment and makes inference results accessible to applications.\\n\\n### Enter ML Metadata Store\\n\\nThis improves the process by introducing a centralized store with an experiment tracker and a model registry.  The tracker records metadata about training, datasets, model parameters, and performance, while the registry manages model binaries, making deployment more streamlined and efficient.\\n\\n### Automating Model Delivery\\n\\nAutomating model deployment minimizes SE involvement.  CI/CD pipelines automatically deploy models triggered by training pipeline steps or Model Registry changes. Note that this usually involves continuous delivery, requiring approval after CI/CD.\\n\\n### Automated ML Training Pipelines and Continuous Training\\n\\nAutomated training pipelines and orchestrators (e.g., Airflow, Kubeflow) are introduced. Pipelines are defined in code (Python), allowing versioning.  Continuous training is possible with proper data validation and pipeline short circuits for safety. \\n\\n### CI/CD for ML Training Pipelines\\n\\nML Training pipelines are now first-class citizens in the CI/CD pipeline, undergoing unit and integration tests. Delivery happens via CI/CD to pre-prod or prod. \\n\\n### Breaking the Remainder of the Wall: CD for Model Deployment\\n\\nTo increase team ownership, Continuous Delivery for ML Models is implemented. This adds ML-specific tests to the deployment pipeline. Deployment might leverage frameworks like Seldon Core or KFServe.\\n\\n### Machine Learning Real-Time Monitoring\\n\\nReal-time monitoring, focusing on ML-specific metrics (feature and model drift), is the final, and optional, step. This monitors model features and inference results to detect drift and trigger retraining.  Robust monitoring systems can be costly, and continuous training often suffices for drift prevention.\\n\\n### Feature Store (Optional)\\n\\nFeature Stores\u2014located between Data Engineering and ML pipelines\u2014offer benefits like eliminating training/serving skew and enabling feature sharing and discoverability through metadata. However, their implementation should be carefully considered due to the complexity of adding this to an existing system.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/sai-28-organisational-structure-for\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34284555-95af-4303-bc05-fd54fb317a4b_1600x818.png\",\"https://www.newsletter.swirlai.com/p/sai-23-deconstructing-a-feature-store\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bfce428-6305-48e2-b4cc-890c9195e573_1600x1439.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e232c69-f779-47b6-af65-0f33e7fb8afb_1600x1439.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04353a76-fd35-40ed-a899-5fecb0f54f1a_1600x1439.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a6f1b2f-32a8-40d2-b0d4-5e00ac09a627_1600x1317.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce445ddc-ec66-41e1-ae17-c33bd768c6f7_1362x1600.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7817308-a8c6-4de5-a497-3d5d9b0f5763_1600x1317.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69de7ed5-84d4-408b-9cad-42c211b8280f_1600x1317.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F604e4ec0-f041-4c47-865c-dba7671142ca_1600x1317.png\",\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbeec7e21-24e1-4815-8db9-1f5f151f9bc9_1600x1317.png\"]}", "{\"title\":\"The SwirlAI Data Engineering Project Master Template.\",\"short\":\"Level up your Data Engineering skills with the SwirlAI Project Template!  Learn real-time & batch pipelines, Kubernetes deployment, & MLOps integration. #DataEngineering #MLOps #Kubernetes\",\"long\":\"### Introduction to the SwirlAI Data Engineering Project Template\\nThis article introduces a comprehensive data engineering project template designed to upskill data engineers.  It covers various data manipulation patterns, from data creation to serving.  Future articles will provide hands-on tutorials on implementing each component.\\n\\n### Benefits of Implementing the Template\\nImplementing this template provides practical experience with real-world data engineering challenges. It allows you to learn various data manipulation patterns and build a production-ready system, showcasing your skills to potential employers.\\n\\n### Real-Time and Batch Pipelines\\nThe template includes both real-time and batch pipelines. The real-time pipeline uses a collector, enricher/validator, enrichment API (potentially including ML models), and real-time and batch loaders. The batch pipeline handles data from the object storage, performing data quality checks, deduplication, and model building.\\n\\n### Scaling the Infrastructure\\nThe template emphasizes scalability and production readiness.  Techniques like horizontal scaling, load balancing, consumer groups, centralized logging (with FluentD and Elasticsearch), and centralized metrics collection (with Prometheus and Grafana) are incorporated.  Alerting mechanisms for data quality issues are also crucial.\\n\\n### The Collector Microservice\\nThe collector acts as a gateway, validating incoming data and adding metadata.  It decouples data producers, allowing for diverse technologies and deployment locations (private or public networks).  It focuses on minimal processing to prevent data loss.\\n\\n### Enricher/Validator\\nThis component validates data schemas against a schema registry, enriches data (possibly with ML inference), and routes valid/invalid data to appropriate streams.  It emphasizes efficient data processing and uses a separate stream for retries.\\n\\n### Enrichment/ML API\\nThis component focuses on integrating machine learning models.  Two patterns are explored: embedding the model within the streaming application and utilizing a decoupled ML service. The article opts for the latter, allowing for flexible model deployment and versioning.\\n\\n### Real-Time and Batch Loaders\\nThese loaders move data to real-time (Elasticsearch) and batch (object storage) systems.  The real-time loader offers immediate analytics, while the batch loader uses efficient serialization (parquet) and compression.\\n\\n### Batch Pipeline\\nThis stage involves data deduplication, quality checks (Great Expectations), and data transformation for downstream systems, using Airflow or other orchestrators for scheduling.  Data modeling is a key component of this pipeline.\\n\\n### Conclusion\\nThis template offers comprehensive upskilling in data engineering, covering various patterns and technologies.\\n\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://github.com/swirl-ai/ai-angineers-handbook\",\"https://www.youtube.com/@swirlai\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project/comments\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project?utm_source=substack&utm_medium=email&utm_content=share&action=share\",\"https://www.newsletter.swirlai.com/i/134943777/introduction-to-the-template\",\"https://www.newsletter.swirlai.com/i/134943777/real-time-pipeline\",\"https://www.newsletter.swirlai.com/i/134943777/some-of-the-infrastructure-elements-that-will-be-needed\",\"https://www.newsletter.swirlai.com/i/134943777/why-might-this-template-be-a-good-idea-to-implement-if-you-want-to-up-skill-as-a-data-engineer\",\"https://www.newsletter.swirlai.com/i/134943777/how-do-we-scale-the-infrastructure-of-the-template-and-make-it-ready-for-production\",\"https://www.newsletter.swirlai.com/i/134943777/the-collector\",\"https://www.newsletter.swirlai.com/i/134943777/enrichervalidator\",\"https://www.newsletter.swirlai.com/i/134943777/enrichmentmachine-learning-api\",\"https://www.newsletter.swirlai.com/i/134943777/embedded-pattern-where-ml-model-is-embedded-into-the-streaming-application-itself\",\"https://www.newsletter.swirlai.com/i/134943777/enricher-pattern-where-streaming-application-uses-a-decoupled-ml-service-to-enrich-the-streamed-data\",\"https://www.newsletter.swirlai.com/i/134943777/real-time-and-batch-loaders\",\"https://www.newsletter.swirlai.com/p/sai-03-machine-learning-deployment#%C2%A7%F0%9D%97%9E%F0%9D%97%AE%F0%9D%97%B3%F0%9D%97%B8%F0%9D%97%AE-%F0%9D%97%A5%F0%9D%97%B2%F0%9D%97%AE%F0%9D%97%B1%F0%9D%97%B6%F0%9D%97%BB%F0%9D%97%B4-%F0%9D%97%97%F0%9D%97%AE%F0%9D%98%81%F0%9D%97%AE-%F0%9D%97%95%F0%9D%97%AE%F0%9D%98%80%F0%9D%97%B6%F0%9D%97%B0%F0%9D%98%80\",\"https://www.newsletter.swirlai.com/i/134943777/closing-thoughts\"]}", "{\"title\":\"Levels of Data Freshness in Machine Learning Systems\",\"short\":\"Learn about the four levels of data freshness in Machine Learning systems.  From offline batch processing to complex online training, I discuss the trade-offs and ROI for each.\",\"long\":\"### Data Freshness\\nData Freshness is the time from data generation until it's available for use. Data Latency is the time from data generation until it's in core storage.  Machine Learning Systems consume data, focusing on Feature Freshness.\\n\\n### Feature Freshness in ML Systems\\nFeature Freshness has two aspects: model training time (how long until data is in the model) and inference time (how long until data is used for prediction).  Both impact overall system freshness.\\n\\n### Levels of Feature Freshness\\nThe article proposes four levels:\\n\\n*   **Level 1 (Offline Training & Batch Inference):**  Simple setup using a data warehouse for training and batch prediction. Model retraining and inference happen on a schedule.\\n*   **Level 2 (Offline Training & Real-Time Inference):** Uses a feature store for both training and real-time prediction, improving inference speed.  Only static features are used.\\n*   **Level 3 (Offline Training & Real-Time Inference with Dynamic Features):** Introduces real-time data processing for dynamic features, enhancing overall freshness. Requires a stream processing system.\\n*   **Level 4 (Online Training & Real-Time Inference):**  Continuously trains and deploys models, providing the highest level of freshness.  Highly complex and expensive to implement.\\n\\n### Moving Between Levels\\nThe author emphasizes starting simple (Level 1) to assess value before investing in more complex solutions.  Level 2 is recommended for most cases, while levels 3 and 4 are for specific needs.\\n\\n### ROI Considerations\\nThe return on investment (ROI) when improving feature freshness depends on additional revenue generated versus the cost of infrastructure and development.  Online Testing helps evaluate revenue improvements.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/sai-16-feature-vs-concept-drift#%C2%A7feature-vs-concept-drift\",\"https://www.newsletter.swirlai.com/p/sai-24-feedback-loops-in-machine\"]}", "{\"title\":\"A Guide to Optimizing Your Spark Application Performance (Part 1)\",\"short\":\"Optimize your Apache Spark applications! This guide covers key techniques: understanding Spark architecture, narrow vs. wide transformations, efficient joins, maximizing parallelism, partitioning, bucketing, and caching.  Boost performance and reduce costs!\",\"long\":\"### Spark Architecture\\nUnderstanding Spark's architecture is crucial for optimization.  It involves a driver program, executors, and a cluster manager (like YARN or Kubernetes).  The driver creates an execution plan, breaking down the job into stages and tasks executed in parallel by executors.\\n\\n### Wide vs. Narrow Transformations\\nNarrow transformations process data locally within partitions, avoiding network shuffles.  Wide transformations require data movement between executors, impacting performance.  Identify and minimize wide transformations.\\n\\n### Broadcast Joins\\nAvoid data shuffles during joins by broadcasting smaller datasets to all executors. Spark 3.x auto-optimizes some joins for broadcasting.\\n\\n### Maximizing Parallelism\\nTune `spark.default.parallelism` and `spark.sql.shuffle.partitions` to match your cluster resources.  Optimize read parallelism by using `spark.sql.files.maxPartitionBytes` to split large files into manageable partitions.\\n\\n### Partitioning and Bucketing\\nPartitioning data during writes enables predicate pushdown, improving read efficiency. Bucketing is a more advanced technique for improved performance of certain operations like joins. \\n\\n### Caching\\nLeverage Spark's lazy evaluation with caching to persist intermediate results, avoiding redundant computations.  Use `.cache()` or `.persist()` methods for in-memory or disk-based caching. Carefully assess the cost of caching versus the benefits.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://github.com/swirl-ai/ai-angineers-handbook\",\"https://www.youtube.com/@swirlai\"]}", "{\"title\":\"SwirlAI Table of Contents\",\"short\":\"40+ SwirlAI episodes condensed!  New Table of Contents organizes 90+ topics on AI Engineering, Data Engineering, MLOps, System Design, Systems Thinking, Building Organizations, Containers/Kubernetes & Career.  Infographic included!\",\"long\":\"### AI Engineering\\nThis section covers building AI agents from scratch, including tool use and fundamental concepts.  It also delves into LLMOps, focusing on observability within the pipeline. \\n\\n### The SwirlAI Data Engineering Project Master Template\\nThis section introduces the SwirlAI Data Engineering Project Master Template, a comprehensive guide for data engineers.\\n\\n### Data Engineering\\nThis in-depth section explores fundamental data engineering concepts such as file formats, data contracts, database management systems (DBMS), and the CAP theorem.  It also covers Spark, Kafka, and stream processing, providing detailed explanations of their architectures and functionalities.  Various aspects of data pipelines and their optimization are also examined. \\n\\n### MLOps\\nThis section focuses on MLOps, covering model deployment strategies (batch, stream, real-time), infrastructure components (feature stores, model registries), and observability techniques for real-time model monitoring. The importance of continuous training, handling training/serving skew, and effective MLOps processes are addressed.  It also details MLOps maturity levels. \\n\\n### System Design\\nThis section explores the design of various systems, including search/recommender systems, website activity trackers, and feature stores, discussing their architectures and design considerations.\\n\\n### Systems Thinking\\nThis section provides a broader perspective on ML systems, emphasizing their role in end-to-end product delivery and organizational structures for effective MLOps implementation.\\n\\n### Building Organisations\\nThis section focuses on organizational strategies for successful MLOps implementation, considering the placement of data roles within the data value chain.\\n\\n### Containers and Kubernetes\\nThis section briefly touches upon the role of containers and Kubernetes in MLOps. \\n\\n### Career\\nThis section offers insights into career paths in Data Engineering and MLOps, providing learning paths and guidance for transitioning into these fields.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part\",\"https://www.newsletter.swirlai.com/p/what-is-ai-engineering\",\"https://www.newsletter.swirlai.com/p/memory-in-agent-systems\",\"https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline\",\"https://www.newsletter.swirlai.com/p/the-swirlai-data-engineering-project\",\"https://www.newsletter.swirlai.com/i/78757169/\ud835\udde5\ud835\uddfc\ud835\ude04-\ud835\uddd5\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\uddf1-\ud835\ude03\ud835\ude00-\ud835\uddd6\ud835\uddfc\ud835\uddf9\ud835\ude02\ud835\uddfa\ud835\uddfb-\ud835\uddd5\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\uddf1-\ud835\uddd9\ud835\uddf6\ud835\uddf9\ud835\uddf2-\ud835\uddd9\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddee\ud835\ude01\",\"https://www.newsletter.swirlai.com/i/79830976/\ud835\udde6\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf2-\ud835\ude03\ud835\ude00-\ud835\udde1\ud835\uddfc\ud835\uddfb-\ud835\udde6\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf2-\ud835\uddd9\ud835\uddf6\ud835\uddf9\ud835\uddf2\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/82616660/\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee-\ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/104922666/data-contracts-in-the-data-pipeline\",\"https://www.newsletter.swirlai.com/i/89634589/\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee-\ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\ude00-\ud835\uddee\ud835\uddfb\ud835\uddf1-\ud835\udde2\ud835\ude02\ud835\ude01\ud835\uddef\ud835\uddfc\ud835\ude05-\ud835\udde7\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf2\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/92554564/acid-properties-in-dbms\",\"https://www.newsletter.swirlai.com/i/95197292/cap-theorem\",\"https://www.newsletter.swirlai.com/i/96550629/lambda-vs-kappa-architecture\",\"https://www.newsletter.swirlai.com/i/97965705/how-are-dbms-database-management-systems-architected\",\"https://www.newsletter.swirlai.com/i/119650808/sql-query-order-of-execution\",\"https://www.newsletter.swirlai.com/p/sai-notes-02-encoding-in-parquet\",\"https://www.newsletter.swirlai.com/i/79830976/\ud835\uddd6\ud835\uddd7\ud835\uddd6-\ud835\uddd6\ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf4\ud835\uddf2-\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee-\ud835\uddd6\ud835\uddee\ud835\uddfd\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\",\"https://www.newsletter.swirlai.com/i/129008282/what-is-a-vector-database\",\"https://www.newsletter.swirlai.com/i/81264008/\ud835\udde6\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf8-\ud835\uddd4\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\",\"https://www.newsletter.swirlai.com/i/82616660/\ud835\udde6\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf8-\ud835\udde6\ud835\uddf5\ud835\ude02\ud835\uddf3\ud835\uddf3\ud835\uddf9\ud835\uddf2\",\"https://www.newsletter.swirlai.com/i/83990106/\ud835\udde6\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf8-\ud835\uddd7\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddef\ud835\ude02\ud835\ude01\ud835\uddf2\ud835\uddf1-\ud835\udddd\ud835\uddfc\ud835\uddf6\ud835\uddfb\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/85364175/\ud835\udde6\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf8-\ud835\udde3\ud835\uddee\ud835\uddff\ud835\uddee\ud835\uddf9\ud835\uddf9\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\uddfa\",\"https://www.newsletter.swirlai.com/i/86924032/\ud835\udde6\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf8-\ud835\uddd6\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\uddfb\ud835\uddf4\",\"https://www.newsletter.swirlai.com/i/88369065/\ud835\udde6\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddf8-\ud835\uddd8\ud835\ude05\ud835\uddf2\ud835\uddf0\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff-\ud835\udde0\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06-\ud835\udde6\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\",\"https://www.newsletter.swirlai.com/p/sai-26-partitioning-and-bucketing\",\"https://www.newsletter.swirlai.com/i/89634589/\ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee-\ud835\udde8\ud835\ude00\ud835\uddf2-\ud835\uddd6\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/78757169/\ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee-\ud835\uddea\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4-\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\",\"https://www.newsletter.swirlai.com/i/81264008/\ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4-\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee-\ud835\uddd5\ud835\uddee\ud835\ude00\ud835\uddf6\ud835\uddf0\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/83990106/\ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee-\ud835\uddd4\ud835\ude01-\ud835\udddf\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\ude01\ud835\udde0\ud835\uddfc\ud835\ude00\ud835\ude01-\ud835\udde2\ud835\uddfb\ud835\uddf0\ud835\uddf2-\ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4\",\"https://www.newsletter.swirlai.com/i/86924032/\ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee-\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee-\ud835\udde5\ud835\uddf2\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\udde6\ud835\ude01\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\uddf2\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/88369065/\ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee-\ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee-\ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\uddd5\ud835\uddee\ud835\ude00\ud835\uddf6\ud835\uddf0\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/91034339/kafka-reliability\",\"https://www.newsletter.swirlai.com/i/106281588/stream-processing-event-vs-processing-time\",\"https://www.newsletter.swirlai.com/i/119650808/watermarks-in-stream-processing-systems\",\"https://www.newsletter.swirlai.com/i/122635988/apache-flink-architecture\",\"https://www.newsletter.swirlai.com/i/91034339/airflow-architecture\",\"https://www.newsletter.swirlai.com/p/sai-27-event-latency-in-data-systems\",\"https://www.newsletter.swirlai.com/i/122635988/mounting-a-feature-store-on-top-of-curated-data\",\"https://www.newsletter.swirlai.com/i/81264008/\ud835\udde0\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\udddf\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4-\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01-\ud835\udde7\ud835\ude06\ud835\uddfd\ud835\uddf2\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/85364175/\ud835\uddd5\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5-\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01-\ud835\udde7\ud835\uddf5\ud835\uddf2-\ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00-\ud835\uddea\ud835\uddee\ud835\ude06\",\"https://www.newsletter.swirlai.com/i/86924032/\ud835\udde6\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa-\ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4-\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01-\ud835\udde7\ud835\uddf5\ud835\uddf2-\ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00-\ud835\uddea\ud835\uddee\ud835\ude06\",\"https://www.newsletter.swirlai.com/i/103523545/implementing-ml-inference-in-streaming-applications\",\"https://www.newsletter.swirlai.com/i/88369065/\ud835\udde5\ud835\uddf2\ud835\uddfe\ud835\ude02\ud835\uddf2\ud835\ude00\ud835\ude01-\ud835\udde5\ud835\uddf2\ud835\ude00\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\ude00\ud835\uddf2-\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01-\ud835\udde7\ud835\uddf5\ud835\uddf2-\ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00-\ud835\uddea\ud835\uddee\ud835\ude06\",\"https://www.newsletter.swirlai.com/i/104922666/the-types-of-ml-model-deployment\",\"https://www.newsletter.swirlai.com/i/106281588/decomposing-real-time-machine-learning-service-latency\",\"https://www.newsletter.swirlai.com/i/88369065/\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01-\ud835\udde6\ud835\uddf0\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf4\",\"https://www.newsletter.swirlai.com/i/91034339/model-deployment-autoscaling\",\"https://www.newsletter.swirlai.com/i/102238126/patterns-for-implementing-business-logic-in-machine-learning-services\",\"https://www.newsletter.swirlai.com/i/85364175/\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\udde5\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\uddf2-\ud835\udde6\ud835\ude01\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\uddf2\ud835\ude00-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\udde7\ud835\uddf6\ud835\uddfa\ud835\uddf2\",\"https://www.newsletter.swirlai.com/i/127397148/machine-learning-model-compression-and-why-you-might-need-it\",\"https://www.newsletter.swirlai.com/i/79830976/\ud835\uddd9\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2-\ud835\udde6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf2\",\"https://www.newsletter.swirlai.com/i/92554564/central-role-of-the-model-registry\",\"https://www.newsletter.swirlai.com/i/81264008/\ud835\uddd8\ud835\ude05\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddf0\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4\",\"https://www.newsletter.swirlai.com/i/82616660/\ud835\uddd8\ud835\ude05\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\uddd8\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\uddff\ud835\uddfc\ud835\uddfb\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\",\"https://www.newsletter.swirlai.com/i/125722351/the-great-divide-of-mlops-tooling-landscape\",\"https://www.newsletter.swirlai.com/i/102238126/feature-platforms\",\"https://www.newsletter.swirlai.com/i/125722351/building-efficient-experimentation-environments-for-ml-projects\",\"https://www.newsletter.swirlai.com/i/82616660/\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\udde2\ud835\uddef\ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddee\ud835\uddef\ud835\uddf6\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude06-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\udde7\ud835\uddf6\ud835\uddfa\ud835\uddf2\",\"https://www.newsletter.swirlai.com/i/83990106/\ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\udde6\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf6\ud835\uddfb\ud835\uddf4-\ud835\udde6\ud835\uddf8\ud835\uddf2\ud835\ude04\",\"https://www.newsletter.swirlai.com/i/103523545/types-of-trainingserving-skew\",\"https://www.newsletter.swirlai.com/i/86924032/\ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9-\ud835\udde2\ud835\uddef\ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddee\ud835\uddef\ud835\uddf6\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude06-\ud835\uddd4\ud835\uddfb\ud835\uddee\ud835\uddf9\ud835\ude06\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\uddf9-\ud835\udddf\ud835\uddfc\ud835\uddf4\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/100740411/feature-vs-concept-drift\",\"https://www.newsletter.swirlai.com/i/107765461/data-quality-in-machine-learning-systems\",\"https://www.newsletter.swirlai.com/i/79830976/\ud835\udde0\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\udddf\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4-\ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\",\"https://www.newsletter.swirlai.com/i/83990106/\ud835\udde6\ud835\ude01\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddf0-\ud835\ude03\ud835\ude00-\ud835\uddd7\ud835\ude06\ud835\uddfb\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddf0-\ud835\uddd9\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\ude00\",\"https://www.newsletter.swirlai.com/i/97965705/how-do-we-define-data-latency-in-ml-systems-when-serving-online-predictions\",\"https://www.newsletter.swirlai.com/i/107765461/continuous-training-ct-in-machine-learning-systems\",\"https://www.newsletter.swirlai.com/p/sai-24-feedback-loops-in-machine\",\"https://www.newsletter.swirlai.com/i/124125660/cicd-for-machine-learning\",\"https://www.newsletter.swirlai.com/i/127397148/thoughts-on-latency-of-feedback-loops-in-machine-learning-applications\",\"https://www.newsletter.swirlai.com/i/95197292/mlops-maturity-levels\",\"https://www.newsletter.swirlai.com/i/96550629/mlops-maturity-level-by-gcp\",\"https://www.newsletter.swirlai.com/i/97965705/what-is-mlops-maturity-level-and-how-do-we-move-there-from-level\",\"https://www.newsletter.swirlai.com/i/91034339/searchrecommender-system-design\",\"https://www.newsletter.swirlai.com/i/100740411/website-activity-tracking-system-design\",\"https://www.newsletter.swirlai.com/p/sai-22-decomposing-the-data-system\",\"https://www.newsletter.swirlai.com/p/sai-23-deconstructing-a-feature-store\",\"https://www.newsletter.swirlai.com/p/sai-25-twitters-recommender-system\",\"https://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to\",\"https://www.newsletter.swirlai.com/i/124125660/ml-systems-in-the-context-of-end-to-end-product-delivery\",\"https://www.newsletter.swirlai.com/i/99317354/why-you-should-learn-data-engineering-and-machine-learning-pipelines\",\"https://www.newsletter.swirlai.com/i/99317354/placing-data-roles-of-on-data-value-chain\",\"https://www.newsletter.swirlai.com/i/104922666/the-data-value-chain\",\"https://www.newsletter.swirlai.com/p/sai-28-organisational-structure-for\",\"https://www.newsletter.swirlai.com/i/124125660/organisational-structure-for-effective-mlops-implementation\",\"https://www.newsletter.swirlai.com/i/99317354/whats-in-kubernetes-for-mlops\",\"https://www.newsletter.swirlai.com/i/97965705/bonus-moving-to-mlops-engineering-from-other-roles\",\"https://www.newsletter.swirlai.com/i/78757169/data-engineers-learning-path\",\"https://github.com/swirl-ai/ai-angineers-handbook\",\"https://www.youtube.com/@swirlai\"]}", "{\"title\":\"SAI Notes #08: LLM based Chatbots to query your Private Knowledge Base.\",\"short\":\"LLM-based chatbots for private knowledge bases:  Why using commercial LLMs directly is problematic & how context retrieval offers a practical, cost-effective alternative. Learn how vector databases and embedding models facilitate accurate and efficient querying of your private data.\",\"long\":\"### Why not use commercial LLMs directly?\\nCommercial LLMs are trained on vast public data, potentially including irrelevant information. Your private knowledge base might contain recent or confidential data absent from the LLM's training set.  Hallucinations (LLMs generating false information) are also a concern.\\n\\n### Alternative approaches: Passing the entire knowledge base\\nThis method involves including your entire knowledge base in the LLM prompt. This faces limitations due to token limits and high costs associated with passing large amounts of data repeatedly.\\n\\n### Fine-tuning an OSS model\\nFine-tuning an open-source LLM on your private data is another option. However, OSS models generally have lower accuracy than commercial options. Fine-tuning requires expertise, time, and resources for hosting and maintenance; it also doesn't fully mitigate hallucinations.\\n\\n### The preferred approach: Context retrieval\\nThis approach leverages embedding models and vector databases for efficient querying.  Your knowledge base is divided into chunks and transformed into vector embeddings, stored in a vector database. Queries are similarly embedded and compared to the database for context retrieval. Finally, the most relevant text chunks are passed to an LLM for an accurate and cost-effective response.\\n\\n### Conclusion\\nCurrent chatbot architectures for private knowledge bases often rely on external context retrieval to overcome LLM limitations like token limits and hallucinations. This approach is cost-effective and requires less specialized LLM expertise.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://www.newsletter.swirlai.com/p/sai-notes-07-what-is-a-vector-database\",\"https://www.newsletter.swirlai.com/p/sai-notes-08-llm-based-chatbots-to/comments\"]}", "{\"title\":\"SAI Notes #07: What is a Vector Database?\",\"short\":\"Unlock the power of vector databases!  Learn how these databases efficiently store, manage, and retrieve vector embeddings for enhanced machine learning applications, from LLMs to recommendation systems.  Also, discover the difference between partitioning and bucketing in Spark for optimized big data processing.\",\"long\":\"### What is a Vector Database?\\nVector Databases excel at handling vector embeddings, which are representations of data (text, images, audio) in a latent space generated by machine learning models.  They efficiently store, update, and retrieve these vectors using Approximate Nearest Neighbour (ANN) search, finding the most similar vectors to a given query.\\n\\n### Writing/Updating Data\\n1.  Choose an embedding model (depending on data type).\\n2. Embed your data (text, images, etc.) using the model.\\n3. Store the resulting vectors in the database, including metadata for filtering.\\n4. The database indexes vectors and metadata separately for fast retrieval.\\n\\n### Reading Data\\nA query typically involves:\\n1.  Embedding the query into the same latent space.\\n2. Optional metadata query to pre-filter results.\\n3. ANN search using a similarity measure (cosine similarity, Euclidean distance, dot product).\\n\\n### Possible Query Paths\\nTwo approaches exist:\\n1. ANN search, then Metadata query.\\n2. Metadata query, then ANN search.\\nEach path offers trade-offs between accuracy and speed. Metadata filtering may remove relevant context but speeds up the process.\\n\\n### Example Use Cases\\nVector Databases are used in:\\n1. **Natural Language Processing:** Providing context to LLMs for question answering.\\n2. **Computer Vision:** Identifying similar images.\\n3. **Recommendation Systems:** Efficiently retrieving candidate items for ranking.\\n\\n### Partitioning vs. Bucketing in Spark\\nPartitioning and bucketing are Spark techniques improving data processing efficiency. Partitioning divides the dataset by specified columns, enabling partition pruning for faster filtering. Bucketing involves creating buckets for rows based on a hash, leading to efficient joins.  Choose partitioning for low-cardinality columns and filtering, while bucketing is better for high-cardinality columns and complex operations like joins.\",\"links\":[\"https://www.linkedin.com/in/aurimas-griciunas\",\"https://github.com/swirl-ai/ai-angineers-handbook\",\"https://www.youtube.com/@swirlai\",\"https://www.newsletter.swirlai.com/p/sai-19-the-data-value-chain\",\"https://www.swirlai.com/talent-collective\"]}", "{\"title\":\"SAI Notes #06: Machine Learning Model Compression\",\"short\":\"Boost your ML model's performance! Learn model compression techniques like pruning, knowledge distillation, and quantization to reduce size and improve latency.  Also, master feedback loops for optimal model improvement. #ML #ModelCompression #MLOps\",\"long\":\"### Machine Learning Model Compression\\n\\nMachine learning models deployed in production must meet operational metrics beyond ML performance.  Inference latency (time to compute results) and model size (memory usage) are critical for efficient deployment, especially in edge computing.  Large models might exceed device capacity, while slow inference may hinder real-time applications like recommendation engines.\\n\\n### Model Compression Methods\\n\\nModel compression techniques improve latency and size, commonly involving:\\n\\n*   **Pruning:** Removing less-important nodes or branches from trees or neural networks.\\n*   **Knowledge Distillation:** Training a smaller \\\"student\\\" model to mimic a larger \\\"teacher\\\" model.\\n*   **Quantization:** Reducing the precision of model parameters, such as weights and biases (e.g., 32-bit float to 8-bit integer).\\n\\n### Feedback Loops in ML\\n\\nFeedback loops are often overlooked in ML planning.  Key considerations:\\n\\n*   **System Readiness:** Ensure that software systems are equipped to collect and process feedback data.\\n*   **Feedback Latency:**  Understand the time delay between inference and metric collection. Types of feedback latency include:\\n    *   **Instant:** Real-time metrics (e.g., click-through rates).\\n    *   **Delayed:** Metrics that take time to accumulate (e.g., revenue).\\n    *   **Human in the Loop:** Manual feedback from users.\\n\\n### Conclusion\\n\\nThe choice of compression methods and feedback loop mechanisms is crucial for successful ML deployments.  Always monitor your model's predictive power in production, ensuring that the optimization doesn't compromise the accuracy required to solve your business problems.\",\"links\":[]}", "{\"title\":\"James Webb Space Telescope's Astonishing Discoveries\",\"short\":\"James Webb Space Telescope's latest discoveries include a detailed view of Sombrero Galaxy, a supermassive black hole consuming matter at a phenomenal rate, and intricate details of two interacting spiral galaxies.  Other insights involve a giant asteroid 'fart', unprecedented images of Vega, and much more!\",\"long\":\"### Sombrero Galaxy\\nThe James Webb telescope's mid-infrared instrument has revealed a smooth inner disk and intricate clumps within the outer ring of the Sombrero Galaxy, offering a more detailed view than visible light images.\\n\\n### Hungry Black Hole\\nAn exceptionally hungry black hole has been identified at the center of a dwarf galaxy from the early universe (1.5 billion years after the Big Bang). It consumes matter at a rate over 40 times faster than previously thought possible, providing insights into the rapid growth of supermassive black holes.\\n\\n### Smooth Operator\\nThe James Webb and Hubble space telescopes have provided an unprecedented image of Vega. Hubble's observation shows a vast, smooth disk of fine dust particles, while Webb's infrared imaging reveals the glow of warm dust in the disk's outer regions, indicating Vega is alone with rocks.\\n\\n### Bloodshot Eyes\\nComposite images from Webb's mid-infrared data and Hubble's ultraviolet and visible light observations reveal intricate details of two interacting spiral galaxies, highlighting areas rich in dust and gas essential for star formation.\\n\\n### Asteroid Fart\\nObservations of Centaur 29P, a mass of rock and ice orbiting between Jupiter and Neptune, reveal a giant asteroid 'fart' consisting of two carbon dioxide jets and one carbon monoxide jet blasting from its surface, offering new insights into space.\",\"links\":[\"string\"]}", "{\"title\":\"Dragonfly: NASA's Ambitious Mission to Titan\",\"short\":\"NASA's Dragonfly mission will send a drone to explore Titan, Saturn's largest moon, in 2034.  Dragonfly will be the first aircraft to fly on another celestial body, searching for signs of life in Titan's unique methane-rich environment. The mission will last three years, utilizing a nuclear power source to explore diverse locations and analyze samples for biosignatures.\",\"long\":\"### Dragonfly Mission to Titan\\nThe Dragonfly mission is a planned NASA mission to send a robotic rotorcraft to the surface of Titan, Saturn's largest moon. It is planned to be launched in July 2028 and arrive in 2034. Dragonfly will be the first aircraft on Titan and is intended to make the first powered and fully controlled atmospheric flight on any moon, with the intention of studying prebiotic chemistry and extraterrestrial habitability. It will use its vertical takeoffs and landings (VTOL) capability to move between exploration sites.\\n\\n### Titan's Unique Environment\\nTitan is unique in having an abundant, complex, and diverse carbon-rich chemistry and a surface dominated by water ice, with an interior water ocean, making it a high-priority target for astrobiology and origin-of-life studies.  The thick atmosphere, four times the density of Earth's, is mostly nitrogen but also contains methane, which creates a reverse greenhouse effect, keeping the surface cold. Despite the cold temperatures, Titan has an active hydrological cycle with flowing rivers, lakes, and seas of liquid methane.\\n\\n### Dragonfly's Design and Capabilities\\nDragonfly is an octocopter with eight propeller blades, allowing for redundancy. It's powered by a Multi-Mission Radioisotope Thermoelectric Generator (MMRTG), a nuclear power source. Its primary instrument is a drill to collect samples of the hydrocarbon sand dunes and analyze them for biosignatures.  The landing zone, in the Shangri-La region, was chosen because it is believed to have once contained liquid water, offering potential for studying early life conditions. The mission is expected to last three Earth years, making numerous flights across Titan's surface.\",\"links\":[\"https://dragonfly.jhuapl.edu/\"]}", "{\"title\":\"SpaceX's Ambitious 2025 Spaceflight Plans\",\"short\":\"SpaceX's 2025 plans include Starship's first orbital return, a new Starlink satellite, and Raptor V3 engines.  Orbital refueling and dual launch towers will enhance capabilities.  Falcon 9 launches will support lunar missions (Astrolab, ispace, Firefly) and the Haven-1 space station.  The ambitious goals face significant technical challenges but promise significant advancements in space exploration.\",\"long\":\"### SpaceX Starship\\nSpaceX plans to push their Starship higher and faster than ever before in 2025.  They aim for the first orbital return and tower catch of the upper stage ship, a significant feat following previous test flights that resulted in explosions.\\n\\n### Starlink Satellite Deployment\\n2025 will see the first payload deployed by a Starship, likely a new, larger, and more capable Starlink satellite. This upgrade promises faster connection speeds over wider areas and potentially eliminates the need for satellite dishes in many locations.\\n\\n### Raptor Engine Upgrade\\nTo enhance Starship's capabilities, SpaceX will introduce Raptor V3 engines. While performance improvements are not dramatic, the design is more streamlined and integrated, aiming for enhanced reliability and robustness.\\n\\n### Orbital Refueling\\nSpaceX plans to achieve orbital refueling in 2025, a complex process involving docking two Starships at over 20,000 kilometers per hour to extend their range for missions beyond Earth orbit, like lunar or Martian expeditions.  Maintaining cryogenic propellant temperatures in the harsh space environment is a crucial technological hurdle.\\n\\n### Other Missions\\nSeveral other launches are anticipated in 2025, including the Astrolab Venturi Flex rover, the Hakuto-R lunar lander, and the Firefly Blue Ghost lunar lander, all using Falcon 9.  In addition, a Crew Dragon will launch the Haven-1 space station, while Astrobotic's Griffin lander is planned for a lunar mission in November.  However, NASA's Viper project has been discontinued.\",\"links\":[\"https://www.businessinsider.com/nasa-ends-viper-project-continues-moon-exploration-2024-12\"]}", "{\"title\":\"SpaceX, Blue Origin Updates and the Hubble Tension\",\"short\":\"SpaceX's Starship is prepping for its 7th orbital test flight, while Blue Origin's New Glenn is nearing its inaugural launch. Meanwhile, the James Webb Telescope confirms Hubble's findings on the universe's expansion, deepening the mystery of the Hubble tension, prompting exploration of early dark energy or exotic particles as possible explanations.\",\"long\":\"### SpaceX Starship\\nSpaceX conducted a full static fire test of their next-generation Starship, paving the way for the 7th orbital test flight. The launch from Starbase is anticipated around January 11th, reverting to earlier morning launch times.  NASA plans to use a specially equipped jet for thermal imaging of the Starship upper stage during its return from space.\\n\\n### Blue Origin New Glenn\\nBlue Origin maintains its New Glenn rocket is on schedule for its first launch before the end of 2024.  The inaugural mission, NG-1, will carry the Blue Ring pathfinder, a versatile system for satellite deployment to specific orbits.  Jeff Bezos confirmed the rocket is on the pad, awaiting regulatory approval.\\n\\n### James Webb Space Telescope\\nThe James Webb Space Telescope's findings have confirmed earlier measurements of the universe's expansion rate, originally made by the Hubble Space Telescope.  This precision allows for more accurate distance calculations and helps to clarify the 'Hubble tension'\u2014the discrepancy between current expansion rate measurements and predictions from the standard cosmological model.  Theories regarding early dark energy or exotic particles are being explored to explain this.\\n\\n### Hubble Tension\\nThe Hubble tension, a mismatch between measurements of the universe's current expansion rate and predictions, remains a mystery. Webb's data, aligning with Hubble's, rules out measurement errors, suggesting that our cosmological models may be incomplete.\",\"links\":[\"https://www.space.com/spacex-starship-test-fire-engines-flight-7\",\"https://en.wikipedia.org/wiki/Starship_flight_test_7\",\"https://www.uchicago.edu/news/explainer-hubble-constant\"]}", "{\"title\":\"Asteroid Apophis: Planetary Defense and the Perils of Space Rocks\",\"short\":\"Apophis, a massive asteroid, will make a close approach to Earth in 2029. The 2013 Chelyabinsk meteor event showed our limited ability to detect smaller asteroids. NASA's DART mission offers hope, but early detection is key to planetary defense.\",\"long\":\"### Apophis: The Asteroid That Will Make a Close Approach to Earth in 2029\\nApophis is a near-Earth asteroid that is wider than three football fields and heavier than 74 Empire State Buildings.  Its name comes from the ancient Egyptian god of chaos and destruction.  Scientists predict a close approach to Earth in 2029, close enough to potentially disrupt geosynchronous communications satellites.\\n\\n### The 2013 Chelyabinsk Meteor Event\\nA smaller asteroid, about two feet in diameter, entered the atmosphere over Russia on December 30, 2014. It exploded in mid-air at four times the speed of sound, causing damage and injuries.  This event occurred with only twelve hours of warning.\\n\\n### The Importance of Early Detection\\nThe Chelyabinsk event highlights the limitations of current asteroid detection methods.  Smaller asteroids, especially those approaching from the direction of the sun, are difficult to detect until they are very close. The angle at which an asteroid enters the atmosphere also affects the impact's destructive power. A steeper angle could lead to far greater damage than the Chelyabinsk event.\\n\\n### NASA's DART Mission\\nNASA's DART mission successfully redirected a smaller asteroid, Dimorphos, demonstrating the possibility of deflecting dangerous asteroids. This success underscores the importance of early detection to ensure a chance of mitigating potential threats.  The challenge remains in detecting smaller, less visible asteroids before they approach Earth.\",\"links\":[\"https://www.youtube.com/watch?v=gSyZ6O-C-1s\"]}", "{\"title\":\"Space Race Heats Up: China's Mars Ambitions and a New Era for NASA\",\"short\":\"China's Tianwen-3 Mars mission aims for sample return by 2031, while NASA's Artemis program faces delays.  Billionaire astronaut Jared Isaacman, nominated by President-elect Trump, could lead NASA into a new era of commercial space exploration, potentially favoring reusable spacecraft and challenging existing approaches.\",\"long\":\"### China's ambitious Tianwen-3 mission\\nChina is planning a Mars sample return mission, Tianwen-3, scheduled for launch in 2028.  The primary goal is to identify signs of ancient life and bring back the first Mars rock by 2031.  Two Long March 5 rockets will be used; one carrying an orbiter and return vehicle, the other, a lander and ascent vehicle.  Potential landing sites include Chryse Planitia and Utopia Planitia, areas with ancient lake beds, deltas, and coastlines, ideal for preserving biosignatures.\\n\\n### NASA's Artemis program faces challenges\\nNASA's Artemis program, aiming for a crewed moon landing by 2030, is facing delays and budget concerns.  The Artemis II mission, initially planned for late 2025, is now delayed until at least April 2026 due to heat shield issues from the Artemis I test flight.\\n\\n### Jared Isaacman, NASA's potential new administrator\\nPresident-elect Donald Trump has nominated Jared Isaacman, a billionaire businessman, pilot, and astronaut, to lead NASA.  Isaacman's experience in private space missions, including the Inspiration4 mission and the Polaris Dawn mission (where he performed the first civilian spacewalk), makes him a unique choice.  His commercial approach and focus on reusable spacecraft like SpaceX's Starship could significantly influence NASA's future direction.  Former NASA administrator Jim Bridenstine and Lori Garver have expressed their support for the nomination.  Isaacman's emphasis on cost-cutting and streamlining operations, along with his vision for a spacefaring civilization, suggests a possible shift in NASA's strategies and priorities.\",\"links\":[\"https://www.space.com/china-tianwen-3-mars-sample-return-mission\",\"https://spacenews.com/chinese-scientists-build-model-mars-atmosphere-to-aid-sample-return-plans/\",\"https://www.cnbc.com/2024/12/04/elon-musk-explains-why-trump-white-house-would-benefit-spacex-starship.html\",\"https://www.foxnews.com/us/41-year-old-who-just-completed-first-civilian-spacewalk-dropped-out-high-school-start-7-billion-business\",\"https://shift4.com/\",\"https://www.drakenintl.com/\",\"https://www.nasa.gov/\"]}", "{\"title\":\"China's $50 Trillion Moon Conquest: A Detailed Plan\",\"short\":\"China unveils a $50 trillion plan to conquer the moon!  A crewed mission is slated for 2029, followed by the construction of an International Lunar Research Station (ILRS) in the 2030s.  Advanced technologies, including reusable rockets and 3D printing, are central to this ambitious endeavor.\",\"long\":\"### China's Ambitious Moon Plans\\nChina is determined to become the second nation to land humans on the Moon, aiming for a crewed mission in 2029.  This isn't just a symbolic gesture; it's part of a larger $50 trillion plan to establish a permanent presence on the lunar surface.\\n\\n### The Three Components of Success\\nThe mission relies on three key components: a heavy-lift rocket (Long March 10), a crew vehicle (Mengzhou), and a lunar lander (Lanyue).  The Long March 10, similar to SpaceX's Falcon Heavy, will be crucial for delivering the substantial payload. The Mengzhou spacecraft, capable of carrying three astronauts, has already undergone an uncrewed test. The Lanyue lander, also under development, will enable two astronauts to reach the lunar surface.\\n\\n### Innovative Technologies\\nChina is also developing advanced technologies, including a new lunar EVA suit demonstrated in 2024 and a lunar rover, to facilitate exploration. The Long March 10 utilizes existing Chinese rocket technology, aiming for a faster transition to active service.  The reusable core boosters will be retrieved using a tether system.  The Chang'e-6 mission successfully landed on the far side of the moon, demonstrating key technologies.\\n\\n### The ILRS Project\\nChina's ultimate goal is to build the International Lunar Research Station (ILRS) at the lunar south pole, a resource-rich region.  The ILRS project, involving multiple robotic missions between 2031 and 2035, will focus on establishing command centers, research facilities, and in-situ resource utilization. The Chang'e-8 mission will play a critical role by testing 3D printing technology that can transform lunar soil into construction materials.\\n\\n### A Long-Term Vision\\nUnlike the Apollo missions, China intends to establish a lasting lunar presence.  This long-term strategy involves multiple robotic missions to explore the moon\u2019s far side and the south pole, which is believed to be rich in water ice.  The project reflects China\u2019s technological advancements and ambition, potentially paving the way for future human colonization.\",\"links\":[\"url1\",\"url2\",\"url3\"]}", "{\"title\":\"Space Race Update: Mars Landings, Voyager 1, and Lunar Volcanoes\",\"short\":\"NASA is learning how to land heavy payloads on Mars using supersonic retro-propulsion, thanks to insights from SpaceX's Falcon 9. Meanwhile, Voyager 1's communications have been restored, Japan is trying again to land on the Moon, and China's lunar samples reveal a surprising volcanic history.\",\"long\":\"### NASA's Secret to Landing on Mars\\n\\nThe most successful landing procedures so far involve either giant inflatable balls or sky crane operations. However, neither is practical for human landing systems on Mars. NASA has been secretly studying how to land much larger payloads on Mars, getting intel from SpaceX, but not in the way you might think. This has nothing to do with Starship.\\n\\n### The Supersonic Transition Problem\\n\\nUntil ten years ago, it was believed you had to slow a vehicle to below the speed of sound before firing retro-propulsion. On Mars, there is not enough atmosphere to land the way we do on Earth, using atmospheric drag. However, NASA now believes supersonic retro-propulsion is the only way to land heavy equipment, habitats, and humans on Mars.\\n\\n### NASA and SpaceX Partnership\\n\\nBeginning in 2014, NASA and SpaceX formed a three-year public-private partnership to study Falcon 9's supersonic retro-propulsion. The SpaceX boosters were outfitted with special instruments to collect data, and NASA learned that a shock front created by the engine firing forms a protective bubble beneath the rocket, which insulates the spacecraft from turbulence and heat.\\n\\n### Voyager 1's Comeback\\n\\nVoyager 1, at 47 years old and 25 billion kilometers from Earth, is alive and well. Communications with the probe were restored after an October blackout.  With its decaying plutonium power supply running low, only four instruments remain operational, all working at lower temperatures than originally designed. A safety feature was tripped due to low power levels, which automatically switched off non-essential systems, including the main X-band transmitter.\\n\\n### Japan's Moon Landing Attempt\\n\\nJapan is taking another shot at landing on the moon in January 2025. Their Resilience lander from ispace arrived at Cape Canaveral for integration into a SpaceX Falcon 9 rocket. This is lunar mission number two for ispace, following their unsuccessful April 2023 attempt, where an altitude sensor was confused by a crater rim, causing the spacecraft to act as if it were closer to the surface than it actually was.\\n\\n### China's Lunar Discoveries\\n\\nAnalysis of samples from China's Chang'e 6 mission to the lunar far side reveals active volcanoes were erupting there 2.8 billion years ago.  The far side has significantly less evidence of volcanism than the near side. This might be related to the massive impact crater that formed the Moon's South Pole-Aitken basin.\",\"links\":[\"https://www.universetoday.com/164444/the-new-mars-landing-approach-how-well-land-large-payloads-on-the-red-planet/\",\"https://www.science.org/doi/10.1126/science.adi1093\"]}", "{\"title\":\"Is NASA's Moon Rocket Doomed to Fail?\",\"short\":\"Leaked info suggests a 50/50 chance NASA's SLS moon rocket will be cancelled.  But don't worry, Artemis might still happen!  With Trump back, efficiency is key; SpaceX's Starship could replace the SLS, though challenges remain.\",\"long\":\"### NASA's SLS Moon Rocket Might Be Cancelled\\n\\nAccording to recently leaked information, NASA's Space Launch System (SLS) moon rocket may be cancelled. This isn't a new idea; critics and fans of the American space program have voiced concerns for years.  The SLS is considered a lost cause due to high costs and inefficiency.\\n\\n### Artemis Moon Landing Might Still Happen\\n\\nHowever, the cancellation of the SLS doesn't automatically mean the Artemis moon landing is off.  A change in plans is necessary, and there are alternative approaches.\\n\\n### Factors Affecting the Decision\\n\\nThree main factors influence this decision: time, money, and the change in US government administration with Donald Trump's return to power.  Trump's focus on efficiency and a desire to surpass China in space exploration could lead to a re-evaluation of the Artemis program.\\n\\n### SLS's Issues and Alternatives\\n\\nThe SLS and associated infrastructure (Gateway lunar space station, Orion capsule) have significant headwinds.  The development cost is astronomical, estimated at $85 billion (adjusted for inflation), exceeding the cost of the Space Shuttle program. The cost per SLS launch to get four people into lunar orbit would be $4 billion. SpaceX Starship, though still in development, offers cheaper and potentially more efficient alternatives.\\n\\n### SpaceX Starship and Other Options\\n\\nSpaceX's Starship presents a more ambitious and potentially more effective approach.  Utilizing existing technology, such as the crew-rated Falcon Heavy and the Vulcan Centaur upper stage, combined with Starship lunar lander could be a more cost-effective approach, potentially even under a $1 billion for a mission.\\n\\n### Challenges and Future Outlook\\n\\nThe biggest hurdle with a Starship-based mission is the return journey from the moon, which demands a significant heat shield and presents engineering challenges.  The decision ultimately depends on balancing cost, time constraints, and the ambition of maintaining American dominance in space exploration, a goal initially set by Donald Trump and Mike Pence.\",\"links\":[\"https://arstechnica.com/science/2024/11/its-looking-50-50-that-nasas-moon-rocket-will-be-canceled/\"]}", "{\"title\":\"Space Race Update: Strange Smells, Starship Launches, and More\",\"short\":\"Strange smell on the ISS, traced to a Russian spacecraft; FAA updates environmental review for increased Starship launches; China tests inflatable space module & retrieves Shijian-19 satellite; Sunita Williams refutes health rumors; Rocket Lab launches two Electron rockets in 24 hours!\",\"long\":\"### Strange Smell on the ISS\\n\\nA foul odor was reported on the International Space Station (ISS), traced to a newly docked Russian Progress cargo spacecraft.  Cosmonauts quickly resealed the airlock to quarantine the area.  The source remains unidentified.  NASA later confirmed the incident.\\n\\n### Starship Updates\\n\\nThe Federal Aviation Administration (FAA) released an updated environmental assessment for increased Starship launches from Starbase, Texas, allowing for up to 25 launches and landings annually. Public comment periods and meetings are scheduled.\\n\\n### China's Space Activities\\n\\nChina successfully completed a mission with its retrievable Shijian-19 satellite, launched on September 27th and returned on October 10th. The satellite conducted microgravity experiments and tested new space technologies, hinting at future inflatable habitats for long-term outposts on the Moon.  Additionally, China quietly tested its first inflatable space module in orbit.\\n\\n### Astronaut Sunita Williams' Health\\n\\nRumors circulated online about astronaut Sunita Williams' health, suggesting malnourishment. Williams herself refuted these claims, stating she weighs the same as at launch and is experiencing typical bodily changes due to microgravity, including shifts in fluid distribution.  She's undertaking an intensive exercise regimen. \\n\\n### Rocket Lab's Achievements\\n\\nRocket Lab successfully launched two Electron rockets within 24 hours, setting a new company record.  These were part of a five-launch contract with Kineis to deploy satellites for IoT services.  Rocket Lab's CEO revised the projected number of Electron flights for 2024 to between 15 and 18, falling short of their initial goal of 22.\",\"links\":[\"https://spacenews.com/iss-crew-reports-unexpected-odor-from-russian-progress-cargo-spacecraft/\"]}", "{\"title\":\"The Challenges and Possibilities of Life on Mars\",\"short\":\"Mars colonization presents unique challenges: space travel, microgravity, thin atmosphere, radiation.  Innovative habitats (Ice Home, inflatable, 3D-printed) are being developed.  Settlers will need diverse skills, AI support, and strong community bonds.  The future might involve terraforming, creating a unique Martian culture, and establishing an independent society.\",\"long\":\"### Life on Mars\\nLife in a Mars colony will be challenging. Settlers will face a six-to-nine-month journey in space, contending with the effects of microgravity on their bodies. Upon arrival, the transition to Mars' weaker gravity will be disorienting.\\n\\n### Mars Environment\\nMars' atmosphere is only 1% the density of Earth's and rich in carbon dioxide. Temperatures plunge to an average of -60\u00b0 Celsius, even colder at the poles. Radiation levels are dangerously high due to the lack of a protective magnetic field.\\n\\n### Mars Habitats\\nTo survive, settlers will rely on habitats with closed-loop life support systems that recycle air, water, and waste.  NASA's Langley Research Center developed the Ice Home, a unique habitat concept built from Martian water ice.  Another innovative concept is the inflatable habitat, easily transported and inflated on Mars. 3D-printed habitats, using Martian regolith, offer automated construction.\\n\\n### Challenges on Mars\\nCommunication between Earth and Mars will be challenging due to the vast distance (4 to 24 minutes delay each way). AI companions and virtual assistants will help settlers cope with this delay.  Other challenges include maintaining physical and mental health in isolation, managing resources, and dealing with Martian dust.  Specialized skills will be crucial, but everyday roles like farming, cooking, and waste management are also vital for a thriving community.\\n\\n### Future of Mars Colonies\\nOver time, Mars colonies will develop their own culture and potentially become independent societies.  Advanced building techniques and resource management will lead to larger, more permanent structures.  Ultimately, Mars could become a thriving civilization.\",\"links\":[\"https://www.nasa.gov/\"]}", "{\"title\":\"SpaceX Starship Test Flight: A Banana in Space and Other Updates\",\"short\":\"SpaceX's Starship season finale delivered stunning visuals and unexpected challenges!  Heatshield modifications and a daring flight profile pushed the limits.  Plus, a banana went to space!  Learn more about the mission's successes and setbacks.\",\"long\":\"### Starship's Season Finale\\nThe latest SpaceX Starship test flight, which was the finale of season one, brought spectacular views and a typical dose of confusion. The video shows high-speed excitement and the launch of the Starship.  The renders of the Human Landing System Starship were also shown, noticeably absent were the solar panels.\\n\\n### Mars Mission Preparations\\nSpaceX is preparing for the first Starship mission to Mars in 2026. They have conducted practical tests of the heatshield material to deal with the Martian atmosphere entry. The Martian atmosphere is much thinner and almost entirely CO2, which will burn up into plasma differently than the Earth's atmosphere.\\n\\n### Starship's Final Launch\\nThe video shows the final launch of the Block 1 Starship.  Two notable changes were made: 2100 heat shield tiles were removed from the sides of the rocket, and tiles were removed from the nose cone and replaced with a layer of stainless steel.  The heatshield was not upgraded to the latest material.\\n\\n### Flight Profile\\nSpaceX aims to increase the angle of attack during subsonic descent, enabling more cross-range capability but increasing stress on control flaps.  The launch appeared clean, but something went wrong during ascent.\\n\\n### Additional Notes\\nThe communications antenna on the launch tower was bent after launch.  The video shows the booster's clean water landing, with less fire than previous attempts.  A banana was sent into space as the first payload carried by a Starship rocket.  A Raptor engine re-light was tested, which will be crucial for future higher-altitude missions.\",\"links\":[\"string\"]}", "{\"title\":\"The Soviet Union's Obsession with Beating NASA\",\"short\":\"The Soviet Space Race: ambition over safety. From Sputnik 1 to the ill-fated Soyuz 1, a story of impressive achievements overshadowed by tragic failures and a reckless pursuit of speed over safety. #SpaceRace #SovietUnion #History\",\"long\":\"### The Soviet Union's Space Program Beginnings\\nThe Soviet Union's space program started similarly to the American program\u2014by acquiring intellectual property from the Nazis, specifically the V-2 rocket technology.  This rocket, initially a long-range ballistic missile, was the first man-made object to reach space, reaching an altitude of 175 kilometers.\\n\\n### The Space Race and Nikita Khrushchev\\nNikita Khrushchev, the Soviet leader who succeeded Joseph Stalin, saw the potential of spaceflight as a powerful propaganda tool to showcase Soviet technological superiority.  His ambition led to the development of the R-7 missile, initially designed for devastating attacks on American cities.\\n\\n### Sputnik 1 and Sputnik 2\\nKhrushchev's ambition fueled the launch of Sputnik 1, a simple radio transmitter in a metal sphere, successfully orbiting Earth.  The following Sputnik 2 mission launched Laika, a dog, into orbit, but she died due to overheating.  This tragic event underscores the Soviets' prioritization of speed over safety in the space race.\\n\\n### The First Manned Spaceflight\\nThe Soviets pressed forward, despite setbacks and significant loss of life, such as the Nedelin Catastrophe.  Yuri Gagarin became the first man in space.  However, even the Vostok program had safety issues.  Alexei Leonov, the first man to conduct a spacewalk, nearly died due to his spacesuit expanding in the vacuum of space.\\n\\n### The Soyuz Program and its Failures\\nThe Soviets' continued pursuit of space dominance led to the development of the Soyuz spacecraft. Despite ambition, the Soyuz 1 and 2 missions ended in disaster, highlighting the program's persistent safety problems. The N1 Moon rocket project, Corolev\u2019s masterpiece, also ultimately failed due to technical issues, marking a significant setback for the Soviet space program.\",\"links\":[\"url1\",\"url2\"]}", "{\"title\":\"Space Race Update: Starship, Sierra Space, Long March 9, and the Roman Coronagraph\",\"short\":\"SpaceX's Starship flight 6 launches November 18th; Sierra Space building 2nd spaceplane; China's Long March 9 resembles Starship; NASA's Roman Coronagraph will search for alien planets.\",\"long\":\"### SpaceX's Starship\\nSpaceX plans the sixth flight test of Starship for November 18, 2024. This flight will have a 30-minute launch window opening at 4:00 p.m. local time. The return of the ship stage will happen in daylight hours over the Indian Ocean, which will provide better conditions for visual observation.  Several thermal protection experiments and operational changes will test the limits of Starship capability. This will be the last flight to feature the classic Starship upper stage design. Flight 7 will transition to the Block 2 Starship with significant upgrades.\\n\\n### Sierra Space\\nSierra Space is moving forward with the construction of their second cargo spaceplane, named Reverence.  The company is also establishing their first mission control center in Colorado.  They are about 18 months away from completion of construction.  Each spaceplane is good for 15 missions.\\n\\n### China's Long March 9\\nA scale model of the latest concept for the Long March 9 rocket was shown at the Zhuhai Airshow. The design is a two-stage reusable heavy lifter, targeting 2033. This is similar to SpaceX's Starship.\\n\\n### NASA's Roman Space Telescope\\nThe Roman Coronagraph has been integrated into the instrument carrier for NASA's Nancy Grace Roman Space Telescope.  This instrument will block starlight to detect faint planets beyond our solar system.  It is scheduled to launch as early as May 2027 and will have a field of view 100 times larger than Hubble. It will be used to search for signs of life on exoplanets.\",\"links\":[\"https://spacenews.com/spacex-plans-next-starship-flight-for-mid-november/\",\"https://spacenews.com/sierra-space-expands-spaceplane-fleet-with-in-house-mission-control/\",\"https://spacenews.com/chinas-new-rocket-for-crew-and-moon-to-launch-in-2026/\",\"https://spacenews.com/nasas-roman-space-telescope-gets-ready-to-stare-at-distant-suns-to-find-alien-planets/\"]}", "{\"title\":\"The New Space Race: Unlocking Venus's Secrets\",\"short\":\"Venus, Earth's twin, holds secrets to our planet's future!  India's VOM, ESA's EnVision, & NASA's DAVINCI will unravel Venus's mysteries, from its fiery surface to its acidic clouds. This coordinated effort will provide insights into planetary evolution and habitability in our galaxy. #Venus #SpaceExploration #ISRO #ESA #NASA\",\"long\":\"### Venus: Earth\u2019s closest planetary neighbor\\nVenus, despite its hellish surface, is remarkably similar to Earth in size and composition. Its surface temperature is high enough to melt lead, the air pressure is crushing, and the atmosphere is thick with sulfuric acid clouds.  However, studies suggest Venus may have had liquid water on its surface for billions of years.\\n\\n### Studying Venus to understand Earth's future\\nUnderstanding Venus's dramatic climate shift could offer clues about Earth's potential future and what makes our planet unique in sustaining life.  Many exoplanets discovered orbiting distant stars have atmospheres and sizes similar to Venus, making its study crucial.\\n\\n### A new era of Venus exploration\\nNASA, ISRO, and ESA are gearing up for a new era of Venus exploration.  Past Soviet missions provided limited data due to the harsh environment and short lifespans of the probes.  New missions aim to gain a deeper understanding.\\n\\n### ISRO's Venus Orbiter Mission (VOM)\\nISRO's VOM, launching in March 2028, is a full scientific mission, unlike its Mars mission which was primarily a technology demonstration.  VOM will use advanced instruments, including a Synthetic Aperture Radar to map the surface and subsurface geology at high resolution, thermal cameras to monitor temperature variations, and atmospheric instruments to study composition and dynamics.\\n\\n### ESA's EnVision mission\\nThe ESA's EnVision mission, scheduled for launch in the early 2030s, will complement VOM. EnVision's payload includes advanced radar and spectrometers to investigate the planet's surface and core layers. It will map the surface at different wavelengths and search for subsurface features.\\n\\n### NASA's DAVINCI mission\\nNASA's DAVINCI mission, launching in 2029, will send a descent probe directly into Venus's atmosphere. Its instruments will measure temperature, pressure, and gas composition in real-time. The data collected from all three missions will create a comprehensive picture of Venus, aiding in the understanding of planetary evolution and habitability.\",\"links\":[\"https://www.jatan.space/\"]}"]